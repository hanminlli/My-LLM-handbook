\documentclass[11pt]{article}  % Sets the font size to 12 pt and document type to article

% =============================
% Essential Packages
% =============================
\usepackage[utf8]{inputenc}     % Ensures proper encoding of special characters
\usepackage[T1]{fontenc}        % Improved font encoding
\usepackage{amsmath, amsfonts, amssymb} % Common math packages
\usepackage{geometry}           % More intuitive control over margins
\usepackage{fancyhdr}           % Custom headers and footers
\usepackage{lmodern}            % Improved Latin Modern font
%\usepackage{setspace}          % Uncomment for line spacing adjustments (e.g., \doublespacing)
\usepackage[most]{tcolorbox}

% =============================
% Color Box
% =============================

\newtcolorbox{notebox}[2][]{%
  colback=yellow!10!white,
  colframe=yellow!50!black,
  coltitle=black,
  fonttitle=\normalfont,
  enhanced,
  attach boxed title to top left={yshift=-2mm,xshift=5mm},
  boxed title style={colback=yellow!50!white},
  title={#2},
  sharp corners=south,
  rounded corners=northwest,
  #1
}

% =============================
% Page Layout
% =============================
\geometry{
  left=1in,
  right=1in,
  top=1in,
  bottom=1in
}

% =============================
% Header Setup
% =============================
\pagestyle{fancy}
\fancyhf{}  % Clear default header/footer
\fancyhead[R]{\today}
\fancyhead[C]{Norm Selection}
\fancyhead[L]{}
\renewcommand{\headrulewidth}{0.4pt}  % Header line thickness


% =============================
% Macros
% =============================
\input{macros.tex}  % No .tex extension needed


% =============================
% Begin Document
% =============================
\begin{document}

\section{Linear Algebra}


\paragraph{Column wise decomposition.}
Any matrix $\mA \in \R^{m \times n}$ can be decomposed into the sum of its columns: 
\begin{align}
	\mA = \sum_{i=1}^{n} \mA_{:j} e_j^{\top},
\end{align}
where $e_j$ are standard basis vectors of $\R^n$.
Notice that this is a rank $1$ decomposition.

\paragraph{Row wise decomposition.}
Any matrix $\mA \in \R^{m \times n}$ can be decomposed into the sum of its rows: 
\begin{align}
	\mA = \sum_{i=1}^{m} e_i \mA_{i:}^{\top},
\end{align}
where $e_i$ are standard basis vectors of $\R^m$.
Notice that this is a rank $1$ decomposition.


\section{LLM Training}
\paragraph{Scaling the logits after LLM head.}
We usually apply RMS norm to normalize (along the last dimension $E$, where it stands for model dimension, $B$ means batch size and $L$ means sequence length) the tensor $\mX \in \R^{B \times L \times E}$ we feed into LLM head, and obtain the corresponding logits $l$.
After RMS normalization, each tensor corresponding to the token $x_i \in \R^E$ will then have ${\rm RMS}(x_t) = 1$.
Now notice that for each coordinate $x_{i, t}, t\in[E]$, treating as a random variable, its variance is given by 
\begin{align}
  \Var\rbrac{x_{i, t}} = \Exp{x^2_{i, t}} - \rbrac{\Exp{x_{i, t}}}^2,
\end{align}
and if it is zero-mean (or small), then $\Var\rbrac{x_{i, t}} \simeq \Exp{x^2_{i, t}}$, which is to say that second moment reflects the variance.

The next step is to use the empirical observation that for linear layers, hidden vectors tend to be approximatedly rotation-invariant (isotropic), i.e., each coordinate behaves like the others, so we can use the second moment over the coordinate in a token to replace the actual second moment.
And the former, is given by 
\begin{align}
  \Var\rbrac{x_{i, t}} \simeq \frac{1}{E}\sum_{t=1}^{E}x_{i, t} = 1.
\end{align}
Now we start to consider the logits, which is generated by 
\begin{align*}
  l_{j, i} = w_j^{\top}x_i = \sum_{t=1}^{E}w_{j, t}x_{i, t}.
\end{align*}
If we assume each weight entry $w_{j, t}$ are i.i.d. with variance $\sigma^2$ the logits variance is give by 
\begin{align*}
  \Var\rbrac{l_{j, i}} = \sum_{t=1}^{E} \sigma^2 \Var \rbrac{x_{t, i}} \simeq E \sigma^2.
\end{align*}
So the standard deviation $\sim \sqrt{E}$.
To ensure that logits do not scale with the model dimension, we scale it by $\sqrt{E}$.



\end{document}