\documentclass[11pt]{article}  % Sets the font size to 12 pt and document type to article

% =============================
% Essential Packages
% =============================
\usepackage[utf8]{inputenc}     % Ensures proper encoding of special characters
\usepackage[T1]{fontenc}        % Improved font encoding
\usepackage{amsmath, amsfonts, amssymb} % Common math packages
\usepackage{geometry}           % More intuitive control over margins
\usepackage{fancyhdr}           % Custom headers and footers
\usepackage{lmodern}            % Improved Latin Modern font
%\usepackage{setspace}          % Uncomment for line spacing adjustments (e.g., \doublespacing)
\usepackage[most]{tcolorbox}
\usepackage{enumitem} 
\usepackage{hyperref}   % for clickable refs
\usepackage{cleveref}   % optional, for "Table 1" style
\usepackage{tikz}
\usetikzlibrary{positioning,shapes,arrows.meta,calc} % <-- added calc

\tikzset{
  block/.style={draw, rounded corners, minimum width=26mm, minimum height=7mm, align=center},
  norm/.style={draw, rounded corners, minimum width=20mm, minimum height=7mm, align=center},
  plus/.style={circle, draw, inner sep=0pt, minimum size=4mm},
}



% =============================
% Color Box
% =============================

\newtcolorbox{notebox}[2][]{%
  colback=yellow!10!white,
  colframe=yellow!50!black,
  coltitle=black,
  fonttitle=\normalfont,
  enhanced,
  attach boxed title to top left={yshift=-2mm,xshift=5mm},
  boxed title style={colback=yellow!50!white},
  title={#2},
  sharp corners=south,
  rounded corners=northwest,
  #1
}

% =============================
% Page Layout
% =============================
\geometry{
  left=1in,
  right=1in,
  top=1in,
  bottom=1in
}

% =============================
% Header Setup
% =============================
\pagestyle{fancy}
\fancyhf{}  % Clear default header/footer
\fancyhead[R]{\today}
\fancyhead[C]{Norm Selection}
\fancyhead[L]{}
\renewcommand{\headrulewidth}{0.4pt}  % Header line thickness


% =============================
% Macros
% =============================
\input{macros.tex}  % No .tex extension needed


% =============================
% Begin Document
% =============================
\begin{document}

\section{Linear Algebra}


\subsection{Column wise decomposition.}
Any matrix $\mA \in \R^{m \times n}$ can be decomposed into the sum of its columns: 
\begin{align}
	\mA = \sum_{i=1}^{n} \mA_{:j} e_j^{\top},
\end{align}
where $e_j$ are standard basis vectors of $\R^n$.
Notice that this is a rank $1$ decomposition.

\subsection{Row wise decomposition.}
Any matrix $\mA \in \R^{m \times n}$ can be decomposed into the sum of its rows: 
\begin{align}
	\mA = \sum_{i=1}^{m} e_i \mA_{i:}^{\top},
\end{align}
where $e_i$ are standard basis vectors of $\R^m$.
Notice that this is a rank $1$ decomposition.


\section{Multithreading}
Per-thread scratch buffers (aka thread-local workspaces) are a very common pattern to cut allocation overhead and lock contention in multi-threaded code.
In this way we do not create lots of short-lived temporaries, and have better cache locality and to avoid false sharing on shared buffers.


\section{LLM Training}
\subsection{Scaling the logits after LLM head.}
We usually apply RMS norm to normalize (along the last dimension $E$, where it stands for model dimension, $B$ means batch size and $L$ means sequence length) the tensor $\mX \in \R^{B \times L \times E}$ we feed into LLM head, and obtain the corresponding logits $l$.
After RMS normalization, each tensor corresponding to the token $x_i \in \R^E$ will then have ${\rm RMS}(x_t) = 1$.
Now notice that for each coordinate $x_{i, t}, t\in[E]$, treating as a random variable, its variance is given by 
\begin{align}
  \Var\rbrac{x_{i, t}} = \Exp{x^2_{i, t}} - \rbrac{\Exp{x_{i, t}}}^2,
\end{align}
and if it is zero-mean (or small), then $\Var\rbrac{x_{i, t}} \simeq \Exp{x^2_{i, t}}$, which is to say that second moment reflects the variance.

The next step is to use the empirical observation that for linear layers, hidden vectors tend to be approximatedly rotation-invariant (isotropic), i.e., each coordinate behaves like the others, so we can use the second moment over the coordinate in a token to replace the actual second moment.
And the former, is given by 
\begin{align}
  \Var\rbrac{x_{i, t}} \simeq \frac{1}{E}\sum_{t=1}^{E}x_{i, t} = 1.
\end{align}
Now we start to consider the logits, which is generated by 
\begin{align*}
  l_{j, i} = w_j^{\top}x_i = \sum_{t=1}^{E}w_{j, t}x_{i, t}.
\end{align*}
If we assume each weight entry $w_{j, t}$ are i.i.d. with variance $\sigma^2$ the logits variance is give by 
\begin{align*}
  \Var\rbrac{l_{j, i}} = \sum_{t=1}^{E} \sigma^2 \Var \rbrac{x_{t, i}} \simeq E \sigma^2.
\end{align*}
So the standard deviation $\sim \sqrt{E}$.
To ensure that logits do not scale with the model dimension, we scale it by $\sqrt{E}$.



\section{Attention}

\subsection{Multihead Self Attention (MHA)}
Consider an input tensor $\mX \in \R^{B \times L \times d_{\text{model}}}$ to an attention layer, where $B$ is the batch size, $L$ is the sequence length, and $d_{\text{model}}$ is the model dimension.

\begin{enumerate}
  \item The first step involves computing queries, keys, and values. 
  We have three matrices, $\mW_q$, $\mW_k$, and $\mW_v \in \R^{d_{\text{model}} \times d_{\text{model}}}$, and simultaneously perform the following operations
  \begin{align}
    \mQ = \mX \mW_{q};\quad \mK = \mX \mW_{k};\quad \mV = \mX\mW_{v}.
  \end{align}
  These operations are vectorized, meaning that for each sequence $b$ in the batch of size B, we do 
  \begin{align*}
    \mQ_b = \mX_b\mW_{q} \quad \forall b \in [B].
  \end{align*} 
  $\mW_q$, $\mW_k$, and $\mW_v$ are trainable parameters shared across the entire batch. 
  The resulting $\mQ$, $\mK$, and $\mV$ have the shape $\R^{B \times L \times d_{\text{model}}}$.

  \item Next, for multihead attention, we reshape $\mQ$, $\mK$, and $\mV$ from shape $\R^{B \times L \times d_{\text{model}}}$ into $\R^{B \times H \times L \times d_{\text{head}}}$, where $H$ is the number of attention heads and $d_{\text{head}}$ is the dimension of each head.
  To achieve this, we first divide $d_{\text{model}}$ into $H$ heads, resulting in shapes of $\R^{B \times L \times H \times d_{\text{head}}}$. 
  Then we rearrange into $\R^{B \times H \times L \times d_{\text{head}}}$.
  Conceptually, each head uses a subset of dimensions from $d_{\text{model}}$ to compute scores between queries and keys along the sequence dimension $L$.
  We will use the following notations $\mQ_h, \mK_h, \mV_h$ to denote the per head tensor in $R^{B \times 1 \times L \times d_{\text{head}}}$ for each head $h \in [H]$.

  \item In the next step, we perform the attention calculation:
  \begin{align*}
    \mS_h &\eqdef \text{Scores}_h(\mQ_h, \mK_h) = \frac{\mQ_h \mK_h^{\top}}{\sqrt{d_{\text{head}}}} + \mM\\
    \mA_h &\eqdef \text{Attention}_h(\mQ, \mK, \mV) = \text{softmax}\rbrac{\mS_h}\mV_h, \quad \forall h \in [H].
  \end{align*}
  The scaled multiplication of $\mQ_h$ and $\mK^{\top}_h$ is vectorized, resulting in $\mS_h \in \R^{B\times 1\times L \times L}$ and $\mS \in \R^{B\times H\times L \times L}$.\footnote{Here $\mS$ is the stack of $\mS_h$ along dimension $H$.}
  Optionally, we could use a mask matrix to mask out certain tokens, an example would be the causual self attention.
  To stabilize the gradients, we element-wise divide raw scores by $\sqrt{d_{\text{head}}}$.
  This scaling choice can be justified because each element of $\mQ_h\mK_h^{\top}$ represents a dot product between vectors of dimension $d_{\text{model}}$. 
  The variance of this dot product scales as $\Var\rbrac{\inner{q_h}{k_h}}\sim d_{\text{model}}\sigma_q^2\sigma_k^2$. 
  Since variance scales quadratically, we divide by $\sqrt{d_{\text{head}}}$.
  The softmax operation turns the scores after masking into probabilities, along the last dimension.\footnote{This is to say that for each $L \times L$ matrix, we softmax every row.}
  Imagine $z = [z_1, \hdots, z_L] \in \R^L$ is a row vector, then essentially, softmax defines the operation:
  \begin{align}
    \sigma(z)_i \eqdef \frac{e^{z_i}}{\sum_{j=1}^{L}e^{z_j}}.
  \end{align}
  Sometimes we use a numerically stable version to replace it 
  \begin{align}
    \tilde{\sigma}(z) \eqdef \frac{e^{z_i - \max(z)}}{\sum_{j=1}^{L}e^{z_j - \max(z)}}.
  \end{align}

  It is worth mentioning that in single head attention (scaled dot product), the complexity of computation is $\cO\rbrac{BL^2d_{\text{model}}}$, while for multihead attention, it is the same since we do $\cO\rbrac{H \times BL^2d_{\text{head}}} = \cO\rbrac{BL^2d_{\text{model}}}$. 

  \item Finally, we concatenate and mix attention outputs from all heads. 
  Concatenation involves first transposing $\mA_h$ to $\R^{B \times L \times H \times d_{\text{head}}}$ and then merging the last two dimensions into $\mA \in \R^{B \times L \times d_{\text{model}}}$. 
  This concatenated result is projected using a matrix $\mW_O$, as follows:
  \begin{align}
    \text{MHA}(\mX) = \mA\mW_{O}. 
  \end{align}
  The final output retains the shape $\R^{B \times L \times d_{\text{model}}}$.
\end{enumerate}
There a bunch of reasons why we are using multi heads instead of scaled dot product attention. 
\begin{itemize}
  \item \textbf{Diversity of learned attention patterns}: Each head learns different attention patterns in parallel. 
  A single attention head computes only one set of attention scores.

  \item \textbf{Subspace specialization}: Instead of operating in $d_{\text{model}}$, each head projects to a lower dimension subspace $d_{\text{head}}$.
  This suggests that each head operates in a distinct feature subspace.

  \item \textbf{Improved gradient flow and representation mixing}: Independent paths improve gradient flow and richness of learned representations.
\end{itemize}
Notice that the computational cost are the \textbf{SAME}!

\subsection{Multi Query Attention (MQA)}
In MQA, different heads have its own query, but share the same key and value.
Specifically, for a head $h$, we have 
\begin{align*}
  \mS_h &\eqdef \text{Scores}(\mQ_h, \mK) = \frac{\mQ_h\mK^{\top}}{\sqrt{d_{\text{head}}}} + \mM \\
  \mA_h &\eqdef \text{Attention}_h (\mQ_h, \mK, \mV) = \text{softmax}(\mS_h)\mV, \quad \forall h \in [H]. 
\end{align*}
This means that for each head $h$, we have a separate $\mQ_h \in \R^{B \times 1 \times L \times d_{\text{head}}}$ and shared $\mK, \mV \in \R^{B \times 1 \times L \times d_{\text{head}}}$.

Compared to standrad MHA, MQA has the following features:
\begin{itemize}
  \item \textbf{Reduced parameter count}: Each head has its own query only, shared key and value.
  \item \textbf{Smaller activation size (memory usage)}: Now $\mK, \mV \in \R^{B \times 1 \times L \times d_{\text{head}}}$, so the activation size is smaller.
  \item \textbf{Reduced KV cache (fast inferencing)}: For transformer based models such as GPT, we generate text one token at a time. 
  To avoid recomputing attention over all previous tokens on every step, we cache $k, v$ (key and value vectors) for all previously seen tokens.
  Specifically, in standard MHA, for each layer and token generated, we neeed $2BHLd_{\text{head}}$ for cached $\mK, \mV$.
  In MQA, we share $\mK$ and $\mV$ so that the cost becomes $2BLd_{\text{head}}$.
  \item \textbf{Minimal accuracy loss.} Used in GPT-3.5, PaLM, LLaMA, etc.
\end{itemize}

\subsection{KV cache}
The motivation for KV caching is to enable efficient inference — both in terms of compute time and memory bandwidth.
At inference time only, autoregressive models input a sequence of tokens $\cbrac{x_t, \hdots, x_{t + L - 1}}$ to generate the next token $x_{t + L}$. 
To avoid recomputing key and value vectors for all previous tokens every time, we cache the $k, v$ pairs corresponding to the tokens $x_t \hdots, x_{t + L - 1}$ in the forward pass.
Then, when generating $x_{t + L + 1}$, we can reuse the vectors for cached $x_{t + 1}, \hdots, x_{t + L - 1}$ and we only need to compute $k, v$ for $x_{t + L}$. 
This mechanism is known as the \textbf{KV cache}.


\subsection{Grouped Query Attention (GQA)}
GQA is like an interpolation between MQA and MHA, where we ask groups of heads to share $\mK, \mV$.
Specifically, let $g(h)$ be a function that maps a head $h$ to its corrsponding group index, then we have 
\begin{align*}
  \mS_h &\eqdef \text{Scores}(\mQ_h, \mK_{g(h)}) = \frac{\mQ_h\mK^{\top}_{g(h)}}{\sqrt{d_{\text{head}}}} + \mM \\
  \mA_h &\eqdef \text{Attention}_h (\mQ_h, \mK_{g(h)}, \mV_{g(h)}) = \text{softmax}(\mS_h)\mV_{g(h)}, \quad \forall h \in [H]. 
\end{align*} 
Benefits:
\begin{enumerate}
  \item Less memory than MHA.
  \item Flexible compute/memory tradeoff by controlling the number of k, v heads.
\end{enumerate}
It is used in LLaMA 2 and Mistral.



\subsection{Multihead Latent Attention (MLA)}
Before we go into details, we need to first differentiate between self attention and cross attention. 
\begin{itemize}
  \item Self attention is the case when Q, K, V comes from the same input sequence. Its typically used in encoder blocks of BERT, GPT, LLaMA, etc, and decoder blocks in GPT, T5, etc.
  \item Cross attention refers to the case when Q comes from one sequence but K, V comes from another sequence. It is typically used in the case that decoder attends to encoder outputs (T5, BART), and the case of vision language models where text attends to image, and perceiver-style latent attention.
\end{itemize}
We can formulate cross attention in the following way: Let $\mZ \in \R^{B \times M \times d_{\text{model}}}$ (expanded from $\R^{1 \times M \times d_{\text{model}}}$.) be a target sequence (queries) and $\mX \in \R^{B \times L \times d_{\text{model}}}$ be a source sequence (keys and values),
\begin{align}
  \mQ = \mZ\mW_q; \quad \mK = \mX\mW_k; \quad \mV = \mX\mW_v.
\end{align}
Notice that $M$ could be different than $L$.
In the case of cross-attention with a latent array, we often have, $M << L$, which significantly reduces computational cost by avoiding full self-attention over the entire input sequence.
We then compute the attention scores and softmax:
\begin{align}
  \mS_h &\eqdef \text{Scores}(\mQ_h, \mK_{h}) = \frac{\mQ_h\mK^{\top}_{h}}{\sqrt{d_{\text{head}}}} \\
  \mA_h &\eqdef \text{Attention}_h (\mQ_h, \mK_{h}, \mV_{h}) = \text{softmax}(\mS_h)\mV_{h}, \quad \forall h \in [H].
\end{align}
Notice that in this case $\mS_h \in \R^{B \times 1 \times M \times L}$ and $\mA_h \in \R^{B \times 1 \times M \times d_{\text{head}}}$ for head $h \in [H]$. 
After concatenation, we result in $\mA \in \R^{B, M \times d_{\text{model}}}$, which is like we are focusing on a smaller sequence.
In MLA, latent vector it self is a learnable parameter and shared accross a batch. 
These latents act like information bottleneck that extract useful features from the long input $\mX \in \R^{B \times L \times d_{\text{model}}}$.



\subsection{Latent Transformer Block}
This is a key design of Perceiver (2021, DeepMind), Set Transformer and efficient transformers for long inputs (e.g., audio, video, documents).
Essentially it can be viewed as cross attention followed by latent self-attention.
Mathematically speaking, we are giving a vector $\mX \in \R^{B \times L \times d_{\text{model}}}$. 
First we are using the latent vector $\mZ \in \R^{1 \times M \times d_{\text{model}}}$ (expanded accross batch dimension.) and the input $\mX \in \R^{B \times L \times d_{\text{model}}}$, we have 
\begin{align*}
  \mA_1 = \text{Attention}(\mX_q = \mZ, \mX_k = \mX, \mX_v = \mX),
\end{align*}
which essentially asks "What should I learn from all of you tokens?".
After this step: each latent now contains information extracted from the input.
Notice that now $\mA_1 \in \R^{B \times M \times d_{\text{model}}}$ is a compressed representation of $\mX$, extracted by the latent array.
Then we do normal self attention on the latent variable:
\begin{align*}
  \mA_2 = \text{Attention}(\mX_q = \mA_1, \mX_k = \mA_1, \mX_v = \mA_1)
\end{align*}
where each latent vector is allowed to look at other latents, share what they learned and refine itself.
\begin{verbatim}
  for each block:
    z = z + CrossAttention(q ← z, k ← x, v ← x)
    z = z + SelfAttention(q ← z, k ← z, v ← z)    
\end{verbatim}
Before we actually feed the $\mZ$ and $\mX$ into the attention block and the final feed forward layer, we first do normlization (LayerNorm in the case of my code).



\subsection{Pre- and Post- Normlization}
In general, there are two ways of doing layer normlization, Post-LN and Pre-LN. 
In the original implementation of transformer, post-LN is used. 
However, pre-LN has become the modern default, which is used in GPT-2/3/4, T5, LLaMA, PaLM, Perceiver, etc.
The benefits of using pre-LN includes the follows:
\begin{itemize}
  \item {\bf (Help gradient flow \& increasing training stability)}: In a deep stack, residual paths carry the untouched signal forward. 
  With Pre-LN, those residual paths also carry unit-variance, zero-mean activations (because they are already normalized). 
  That keeps gradients well-scaled and prevents the exploding / vanishing issues that appeared when stacking 24 - 100+ layers with Post-LN.
  Empirically, Pre-LN lets you train hundreds (even thousands) of layers with a stable learning rate schedule, whereas Post-LN often needed warm-up tricks or gradient clipping.
  \item {\bf (Easier optimization of very long sequences)}: Cross-entropy loss is applied after the final LayerNorm.
  With Post-LN every sub-layer's output is renormalized, the network must constantly “undo” those shifts.
  Pre-LN leaves the residual branch untouched, so the model can accumulate information across time steps or tokens without repeatedly rescaling it.
  \item {\bf (Faster convergence)}: Many ablations show ~ 1.3 - 1.5x faster convergence for GPT/T5 style models when switching from Post-LN → Pre-LN. This is because every tensor that flows straight down the stack (both forward activations and backward gradients through the residual skip) has mean 0 and variance 1, which helps stabilize second-moment estimate quickly for Adam.
  \item {\bf (Safer with half-precision / mixed-precision)}: Normalizing before the high-variance matrix multiplications keeps activations in a narrower numeric range, reducing overflow/underflow risk in FP16/BF16 training.
\end{itemize}


\textbf{LayerNorm:} Mathematically speaking, consider an input $\mX$ in the space $\R^{B \times L \times d_{\text{model}}}$, for the $l$-th token in the $b$-th batch
\begin{align*}
  \mL[b, l, :] = \text{LayerNorm}(\mX[b, l, :]) = \gamma \cdot \frac{X[b, l, :] - \mu_{n, l}}{\sqrt{\sigma^2_{b, l}} + \epsilon} + \beta, 
\end{align*}
where 
\begin{align*}
  \mu_{b, l} = \frac{1}{d_{\text{model}}}\sum_{i=1}^{d_{\text{model}}}\mX[b, l, i], \quad \sigma^2_{b, l} = \frac{1}{d_{\text{model}}}\sum_{i=1}^{d_{\text{model}}}\rbrac{\mX[b, l, i] - \mu_{b, l}}^2,
\end{align*}
$\gamma, \beta \in \R^{d_{\text{model}}}$ are learned shift and scale vectors.
Notice that statistics are computed per sample, per position, no batch coupling, so the network behaves the same in training and inference and is robust to batch-size 1. 
Its benefits includes: 
\begin{itemize}
  \item {\bf (Zero-mean, unit-var features).} Keeps dot-products in a predictable range, resulting in stable softmax gradients.
  \item {\bf (Identical behaviour in training / inferencing).} Important for autoregressive generation where batch size changes.
  \item {\bf (Works with any sequence length).} No running-average statistics needed.
\end{itemize}
In a simpler form, layernorm can be written as 
\begin{align*}
  \mL = \gamma \odot \frac{\mX - \mu}{\sqrt{\sigma + \epsilon}} + \beta,
\end{align*}
where all operations are elementwise.
Notice that $\gamma, \beta \in \R^{d_{\text{model}}}$ are learnable parameters.


\subsection{Dropout}
\paragraph{During Training:}
This refers to During training we randomly set a fraction of the sub-layer output activations to zero and scale the survivors.
Specifically, for $\mX \in \R^{B \times L \times d_{\text{model}}}$ and the drop out rate $0 < p < 1$, we sample a binary traning mask $\mM \in \cbrac{0, 1}^{B \times L \times d_{\text{model}}}$, using the Bernoulli($q$) distribution where $q = 1 - p$, we then apply 
\begin{align*}
  \widehat{\mX} = \frac{1}{q}\mX \odot \mM.
\end{align*}
$q$ is referred to as the keep probability, and we are trying to make $\Exp{\widehat{\mX}} = \mX$.
\paragraph{During Inferencing:}
At inference time the mask is removed and $\mX$ passes through unchanged. Code: 
\begin{verbatim}
  torch.nn.Dropout(p=p)
\end{verbatim}

\subsection{DCA}
This one seems to be less well known, on medical image tasks.

\subsection{Linear Attention}
Let us first look at the asymptotic time complexity and memory complexity of attention.
\begin{table}[H]
\label{table1}
\centering
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{@{}l c@{}}
\toprule
\textbf{Operation} & \textbf{Complexity} \\
\midrule
Projections $\mX \mapsto (\mQ,\mK,\mV)$ & $\cO\left(BLd_{\text{model}}^{2}\right)$ \\
Score computation $\mQ\mK^{\top}$ (per head) & $\cO\left(BHL^{2}\dhead\right)$ \\
Softmax over scores & $\cO\left(BHL^{2}\right)$ \\
Weighted sum $\mathrm{softmax}(\mQ\mK^{\top})\mV$ & $\cO\left(BHL^{2}\dhead\right)$ \\
Output projection (concat heads $\to d_{\text{model}}$) & $\cO\left(BLd_{\text{model}}^{2}\right)$ \\
\bottomrule
\end{tabular}
\caption{Computation complexity of the main steps in a standard self-attention block. 
Here $B$ = batch size, $L$ = sequence length, $H$ = number of heads, $\dhead$ = head dimension, and $d_{\text{model}} = H \cdot \dhead$.}
\end{table}
For naive self-attention, we can see that as sequence length becomes longer, the dominating term would be $\cO(BHL^2\dhead)$, which becomes $\cO(BHL_qL_k\dhead)$, which is quadratic in $L$.

\paragraph{Memory consumption:} 
\begin{itemize}
  \item Store $\mQ, \mK$ and $\mV$: $\cO(BL\dmodel)$
  \item Store attention scores/probs: $\cO(BHL^2)$
  \item Output activations for backprob add similar terms.
  \item By FlashAttention / PyTorch SDPA (streaming): we keep compute the same $\cO(BHL^2\dhead)$ but reduce memory from $\cO(BHL^2)$ to roughly $\cO(BL\dmodel)$ by not materializing the full $L \times L$ matrix. 
\end{itemize}

\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Scenario} & \textbf{Time (leading term)} & \textbf{Memory (leading term)} \\
\midrule
Self-attention (train) & $O\!\left(BHL^{2}d_{\text{head}}\right)$ & $O\!\left(BHL^{2}\right)$ \\
Cross-attention & $O\!\left(BHL_{q}L_{k}d_{\text{head}}\right)$ & $O\!\left(BHL_{q}L_{k}\right)$ \\
With FlashAttention / SDPA & same time & $O\!\left(BLd_{\text{model}}\right)$ \\
Autoregressive decoding (per token at step $t$) & $O\!\left(Htd_{\text{head}}\right)$ & cache $O\!\left(Ld_{\text{model}}\right)$ \\
\bottomrule
\end{tabular}
\caption{Asymptotic compute and memory complexity for different attention scenarios. 
Here $B$ = batch size, $L$ = sequence length (train), $H$ = number of heads, $d_{\text{head}}$ = per-head dimension, $d_{\text{model}} = H \cdot d_{\text{head}}$, $L_q$/$L_k$ = query/key lengths for cross-attention, and $t$ = current decoding step in autoregressive inference. Autoregressive decoding is with KV cache.}
\end{table}

\paragraph{Linear attention:} The linear attention is introduced to avoid the quadratic dependence using attention kernels which allow us to obtain linear dependence. 
Note that the attention of token $i$ is given by 
\begin{align*}
  \text{attn}_i = \frac{\sum_{j=1}^{L}\exp\rbrac{\frac{\mq_i^{\top}\mk_j}{\sqrt{\dhead}}}\mv_j}{\sum_{j=1}^{L}\exp\rbrac{\frac{\mq_i^{\top}\mk_j}{\sqrt{\dhead}}}}.
\end{align*}
To compute it, we need to perform $\mQ\mK^{\top}$ and apply softmax rowwise.
We introduce the kernel trick just as we did in SVM, imagine we can write 
\begin{align*}
  \exp\rbrac{\frac{\mq_i^{\top}\mk_j}{\sqrt{\dhead}}} \approx \phi(\mq_i)^{\top} \phi(\mk_j),
\end{align*}
for some feature map $\phi: \R^{\dhead} \mapsto \R^{d_{\phi}}$ with non-negative outputs, then we have
\begin{eqnarray*}
  \text{Numerator:} \qquad && \sum_{j=1}^{L} \phi(\mq_i)^{\top}\phi(\mk_j) \mv_j^{\top} = \rbrac{\sum_{j=1}^{L}\mv_j\phi(\mk_j)^{\top}} \cdot \phi(\mq_i), \\
  \text{Denominator:} \qquad && \sum_{j=1}^{L} \phi(\mq_i)^{\top}\phi(\mk_j) = \rbrac{\sum_{j=1}^{L}\phi(\mk_j)^{\top}} \cdot \phi(\mq_i).
\end{eqnarray*}
Now denote $\mS_\mV \eqdef \sum_{j=1}^{L}\mv_j\phi(\mk_j)^{\top} \in \R^{\dhead \times d_{\phi}}$, and $\mS_{\mK} \eqdef \sum_{j=1}^{L}\phi(\mk_j)^{\top} \in \R^{1 \times d_{\phi}}$, we would have 
\begin{align*}
  \text{attn}_i = \frac{\mS_\mV \phi(\mq_i)}{\mS_\mK\phi(q_i) + \epsilon}.
\end{align*}
Notice that we may compute $\mS_{\mV}$ and $\mS_{\mK}$ in an linear fasion dependent on $\cO(L)$ (since there are in total $L$ terms).
In this way we get a better dependence.

\paragraph{Causal (autoregressive) case:}
All we have to change is that we maintain running sum: at time (token) $t$:
\begin{align*}
  \mS_{\mV}^{(t)} \eqdef \sum_{j\leq t}\mv_j\phi(\mk_j)^{\top} \qquad \text{and} \qquad \mS_{\mK}^{(t)} \eqdef \sum_{j\leq t}\phi(\mk_j)^{\top}.
\end{align*}
and 
\begin{align*}
  \text{attn}_t = \frac{\mS_\mV^{(t)} \phi(\mq_i)}{\mS_\mK^{(t)}\phi(q_i) + \epsilon}.
\end{align*}

There are a bunch of common choices for the feature maps, 
\begin{enumerate}
  \item \textbf{Positive feature maps (kernel trick for softmax)}: (1) ELU + 1 ($\phi(x) = \text{ELU}(x) + 1$), (2) ReLU / Square ReLU, (3) Exp with scaling $\phi(x) = \exp\rbrac{\frac{x}{d_{\text{head}}}}$, (4) Random Fourier features ...
  \item \textbf{Other maps}: Orthogonal / normalized maps; Exponential-normalized maps ...
\end{enumerate}



\subsection{Sparse Attention}


\section{Normalizations}
Besides the layer norm mentioned in the previous chapter, there are other norms used quite often.

\subsection{BatchNorm}
Batch norm is rarely used in language modeling especially in modern architectures. 
It is not sequence or position aware, and it requires consistent statistics over a batch, with variable-length sequences in language modeling, the batch statistics can be unstable and unreliable.
Furthermore, it mixes information from each sequences, but ideally we want to keep it separate. 
However, it is great for vision tasks

Mathematically, given an input $\mX \in \R^{B \times d}$, for each feature $j$, we have 
\begin{align*}
  \mu_j = \frac{1}{B}\sum_{i=1}^{B}\mX[i, j], \quad \sigma^2_j = \frac{1}{B}\sum_{i=1}^{B}(\mX[i, j] - \mu_j)^2.
\end{align*}
Then each value is normlized in a way that 
\begin{align*}
  \hat{X}[i, j] = \gamma[j] \frac{X[i, j] - \mu_j}{\sqrt{\sigma_j^2 + \epsilon}} + \beta[j],
\end{align*}
where $\gamma, \beta \in \R^d$ are learnable parameters per feature / channel.

If we are given an input 2D/Convolutional data $\mX \in \R^{B \times C \times H \times W}$, we have 
\begin{align*}
  \mu_c = \frac{1}{BHW}\sum_{b,h,w}\mX[b, c, h, w], \quad \sigma_c^2 = \frac{1}{BHW}\sum_{b,h,w}(\mX[b, c, h, w] - \mu_c)^2,
\end{align*}
basically, we are computing mean and variance per channel.
Notice that \textbf{LayerNorm is still the default in most Vision Transformer (ViT-style) backbones}. 
In CNN, sometimes GroupNorm replaces BatchNorm because it's batch-size independent and mixes well with vision features.


\subsection{RMSNorm}
Its name is Root Mean Square Layer Normalization, which has become popular in some recent LLMs, especially as a lighter and sometimes more numerically stable alternative to LayerNorm.
It normalizes only by root mean square of the features\footnote{Features in this context refer to the elements in dimension $d_{\text{model}}$} instead of mean and variance.
Given $X \in \R^{B \times L \times d_{\text{model}}}$ 
\begin{align*}
  \text{RMS}_{b, l} = \sqrt{\frac{1}{d_{\text{model}}}\sum_{i=1}^{d_{\text{model}}} \mX[b, l, i]^2 + \epsilon}
\end{align*}
Then, each element in $\mX$ is normalized by 
\begin{align*}
  \hat{X}[b, l, i] = \frac{\mX[b, l, i]}{\text{RMS}_{b, l}} \cdot \gamma_i, \qquad \forall i \in [d],
\end{align*}
where $\gamma_i$ is the $i$-th component of the learnable scaling factor $\gamma \in \R^{d_{\text{model}}}$.

LLaMA (and most of Meta-Llama 1/2/3 checkpoints) swap the original LayerNorm for pre-norm RMSNorm.

Below is a complete list of RMSNorm's features.
\begin{itemize}
  \item {\bf (Rescales by RMS only \& scaling only)}.
  \item {\bf (Compute \& Memory efficiency)}. ~ 30 \% cheaper per norm op; overall 2-6 \% faster end-to-end in large LLMs.
  \item {\bf (Numerically stable)}. Works well with very deep pre-norm Transformers.
\end{itemize}
When does it shine: (1) Ultra-deep LLMs, (2) Inference-first or edge deployments, (3) Pre-norm architectures.

RMSNorm really does skip the “subtract-the-mean” step, so a single normalization needs one less reduction-operation and a bit less memory traffic (removing the mean saves one vector reduction, one broadcast, and one add.). 
This skip would not affect training stability, because pre-norm residuals absorb the offset: in modern transformers RMSNorm sits before each residual branch, any mean shift can be compensated by the next linear layer's bias.
To see this: consider the following procedure, (assuming $x \in \R^d$)
\begin{align*}
  \boxed{x} \xrightarrow{\text{RMSNorm}} \boxed{y = \gamma \cdot \frac{x}{\norm{x}_{\text{RMS}}}} \xrightarrow{\mW, b} \boxed{u = \mW y + b} \xrightarrow{} \boxed{z = x + u}
\end{align*}
$y$ indeed does not have mean zero, denote it as $\mu_{y}$, since the next operation is affine, we can rewrite it as 
\begin{align*}
  u = \mW(y - \mu_{y}{\bf{1}}) + (b + \mW\mu_{y}{\bf{1}}),
\end{align*}
which means that it is equivalent to adjusting the bias. 
During training, back-prop will simply nudge $b$ so the network learns whatever overall shift is optimal, it does not care where that shift originates.
After that $u$ is added to the original $x$, but the drift in $\mu_{y}$ will not accumulate unchecked because: (1) The residual path still carries the original, unshifted activations, (2) The next block starts with another RMSNorm, which rescales its input (including any offset) back to a controlled RMS before new computations begin.



\section{Positional Embedding}




\section{Precisions}
Here is a summary provided by Chat-GPT o3.
% ---------------- Table 1: high / mid precision ----------------
\begin{table}[ht]
\centering
\renewcommand{\arraystretch}{1.15}
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Format} & \textbf{Bits} & \textbf{Exp/Mant} &
\textbf{Dynamic range$^\dagger$} & \textbf{ULP@1} &
\textbf{Typical use} \\
\midrule
FP32        & 32 & 8/23 & $1.2\!\times\!10^{-38}$-$3.4\!\times\!10^{38}$ & $2^{-23}$ & Master weights, optimiser \\
FP16 (IEEE) & 16 & 5/10 & $6.1\!\times\!10^{-5}$-$6.6\!\times\!10^{4}$   & $2^{-10}$ & Fwd/Bwd on Volta \& T4 \\
\textbf{bfloat16} & 16 & 8/7  & $1.2\!\times\!10^{-38}$-$3.4\!\times\!10^{38}$ & $2^{-7}$  & Default on A100/H100, TPU \\
TF32$^{*}$  & 19 & 8/10 & $\phantom{0}$same as FP32 & $2^{-10}$ & GEMMs on Ampere \\
\bottomrule
\end{tabular}
\caption{Mid-/high-precision dtypes widely used during LLM training.%
$^\dagger$Smallest positive \emph{normal} value to largest finite.%
$^{*}$TF32 is a compute mode; tensors stored as FP32.}
\end{table}

% ---------------- Table 2: low precision & INT -----------------
\begin{table}[ht]
\centering
\renewcommand{\arraystretch}{1.15}
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Format} & \textbf{Bits} & \textbf{Exp/Mant} &
\textbf{Dynamic range$^\dagger$} & \textbf{ULP@1} &
\textbf{Typical use} \\
\midrule
FP8-E4M3 & 8 & 4/3 & $9.2\!\times\!10^{-8}$-$4.5\!\times\!10^{2}$  & $2^{-3}$ & Research train / fast inf.\ (H100) \\
FP8-E5M2 & 8 & 5/2 & $3.0\!\times\!10^{-10}$-$5.7\!\times\!10^{4}$ & $2^{-2}$ & Wider range variant (H100) \\
INT8 / INT4 & 8/4 & — & $\pm127$ / $\pm7$ & 1 / $1/16$ & Post-training inference quant. \\
\bottomrule
\end{tabular}
\caption{Very low-precision formats used for efficient training research (FP8) or deployment quantisation (INT).}
\end{table}

A floating point number in IEEE-style formats is stored as:
\begin{verbatim}
  Sign bit | Exponent bits | Mantissa (fraction) bits
\end{verbatim}
\begin{enumerate}
  \item \textbf{Sign bit}: ($1$ bit), $0$ positive and $1$ negative.
  \item \textbf{Exponent bits}: control the scale (powers of $2$). 
  More exponent bits means wider dynamic range.
  \item \textbf{Mantissa bits}: control the precision (how many distinct numbers you can represent between powers of 2). More mantissa bits means smaller ULP\footnote{Unit in the Last Place: The smallest possible difference between two representable floating-point numbers around a given value} (more precise).
\end{enumerate}
A number is typically constructed in this way:
\begin{align*}
  (-1)^{\text{sign}} \times (1.\text{mantissa bits}) \times 2^{\text{exponent} - \text{bias}}.
\end{align*}

\begin{itemize}
  \item \textbf{FP32} is used as the safe baseline in deep learning, it is almost always used for the master weigts\footnote{Master weight refers to the full-precision copy of the model's parameter that we keep during mixed-precision training.} and optimizer state because it is numerically stable.
  
  \item \textbf{FP16} is used in forward/backward pass to save memory in Volta(Nvidia V100)/T4 mixed-precision training. Prone to overflow/underflow unless scaled (hence loss scaling).
  
  \item \textbf{Loss scaling} refers to the practice we multiply our loss by a large constant $S$ before back propagation: $L_{\text{scaled}} = S \cdot L$, this scales up gradients which avoids gradient underflow. 
  However, after computing the gradients, we divide them by $S$ before updating the weights to restore correct magnitude. 
  NVIDIA's AMP (Automatic Mixed Precision) does dynamic loss scaling, adjusting $S$ automatically to prevent overflow. 

  \item \textbf{bfloat16} is Brain floating point came from Google Brain, whose exp bits are same as \textbf{FP32}, but the mantissa bits are much fewer, this avoids the over/under-flow problem in \textbf{BF16}. 
  This comes at a price of lower precision, but it is the default choice on A100 and H100.

  FP16 on Volta/T4 is fast but fragile, and requires loss scaling, which is why bfloat16 became popular. 
  It enjoys the same range as FP32, but no scaling headaches.

  Its advantage includes: (i) no losss scaling needed to handle with over/under flow, (ii) half the memory of \textbf{FP32}, (iii) speed up in matmul on A100/H100, (iv) can used as a drop in for \textbf{FP32}, because the dynamic range is the same.

  For downsides: (a) precisiob loss, (b) optimizers often still keep master weights in \textbf{FP32} to avoid cumulative rounding error, (c) numerically sensitive operations such as $\text{softmax}$/ normalization, kernel still uses \textbf{FP32} internally.

  \item \textbf{TF32}: NVIDIA introduced with Ampere GPUs (A100, RTX 3000 series), default in cuBLAS/cuDNN matmul on Ampere if you pass FP32 inputs. FP32's size and range, FP16's precision.
  
  \item \textbf{FP8-E4M3, FP8-E5M2}: We can now tell directly from their names that FP8-E4M3 has a narrower dynamic range at a higher precision. For FP8-E4M3, ULP@1 is $\frac{1}{2^3} = 0.125$ and for FP8-E5M2, the ULP@1 becomes $\frac{1}{2^2} = 0.25$ which is very coarse.
  
  E4M3: good for weights and activations that stay within a moderate range.

  E5M2: good for gradients or loss-related values that can swing wildly in magnitude.
\end{itemize}

\paragraph{How they (FP8) are used on H100:} It can be applied both for training and inferencing.

During \textbf{training:}
\begin{enumerate}
  \item Keep master weights in FP32 (like with FP16/bfloat16 training).
  \item Cast activations/gradients to FP8 for GEMMs inside the forward/backward pass.
  \item Apply per-tensor or per-channel \textbf{scaling} to map values into FP8's limited range.
  \item Often mix E4M3 for forward activations, E5M2 for backward gradients.
\end{enumerate}

During \textbf{Inferencing:}
Post-training quantization to FP8 for ultra-fast inference with minimal memory footprint.

\paragraph{Scaling is not optional:} FP8's numeric range is so tiny that, without actively scaling tensors before casting to FP8, we'll either hit overflow or underflow constantly.
The fix is that we multiply the tensor by a scaling factor S before FP8 conversion,
\begin{align*}
  x_{\text{scaled}} = x \times S.
\end{align*}
We may choose $S$ so that $\max{x_{\text{scaled}} }$ fits nicely into FP8's max normal value, and we store $S$ as a separate FP32 number. We can later undo the the scaling after computation:
\begin{align*}
  y = y_{\text{scaled}} \times S^{-1}.
\end{align*}
This keeps numbers inside FP8's safe zone while still preserving the original magnitude relationship.

\paragraph{Per channel vs. per tensor:}
Often, there are \textbf{per channel} scaling and \textbf{per tensor} scaling. Let us image that we are doing a fully-connected layer, where we have this weight matrix $\mW \in \R^{d_{\text{out}} \times d_{\text{in}}}$.
Per channel scaling means that we have a scaling factor for each output feature (each row), so there will be $d_{\text{out}}$ scalars, while for per tensor scaling, we only have $1$ scaling factor. In this sense, per tensor scaling is simpler, but will be suboptimal is some channels are loud and the others are quiet, while per channel scaling has better precisions, especially in convolutional layers or transformers where variance differs accross each head / filter.

\paragraph{Automation on H100:} Notice that there is automated scaling on NVIDIA H100, which tracks running max values for each tensor, and picks E4M3 (more precision) for forward activations, E5M2 (more range) for backward gradients.
The engine applies scaling transparently, so we mostly see FP8 without manually tuning the scaling factor $S$.
 
As an example of scaling, let us consider the previous example of fully connected layer, where we are expected to do $\mO = \mW\mX$, originally, both $\mW$ and $\mX$ are FP32 (master weights and full precision activations). 
Before GEMM\footnote{GEneral Matrix-Matrix Multiplication, in the form of $\mD = \alpha\cdot\mA\mB + \beta\mC$, where $\alpha, \beta$ are scalars. This is the workhorse of modern deep learning.}, we independently scaled them to FP8 (to fit the range) by 
\begin{align*}
  \mX_{\text{scaled}} = \mX \cdot S_{\mX}; \qquad \mW_{\text{scaled}} = \mW \cdot S_{\mW},
\end{align*}
then we performed the matmul in FP8, 
\begin{align*}
  \mO_{\text{scaled}} = \text{FP8GEMM}(\mW_{\text{scaled}}, \mX_{\text{scaled}}).
\end{align*}
To restore the original magnitude, we take advantage of the scaling factor $S_{\mX}, S_{\mW}$ who are FP32,
\begin{align*}
  \mO = \mO_{\text{scaled}} \cdot \frac{1}{S_{\mX}S_{\mW}}.
\end{align*}





\section{Encoder and Decoder Structure}

\paragraph{Encoder}
Typically, encodes looks like:

\begin{figure}[ht]
\centering

% ---------------- Pre-LN ----------------
\begin{minipage}{0.45\textwidth}
\centering
\begin{tikzpicture}[>=Latex, node distance=12mm, font=\small]
\node (x)   {$\mX$};
\node (ln0) [norm, below=of x] {LayerNorm};
\node (msa) [block, below=of ln0] {Multi-Head\\Self-Attention};
\node (add1)[plus,  below=of msa] {\small $+$};
\node (ln1) [norm,  below=of add1] {LayerNorm};
\node (ffn) [block, below=of ln1] {Feed-Forward\\Network};
\node (add2)[plus,  below=of ffn] {\small $+$};
\node (y)   [below=of add2] {$\mY$};

% main path
\draw[->] (x) -- (ln0);
\draw[->] (ln0) -- (msa);
\draw[->] (msa) -- (add1);
\draw[->] (add1) -- (ln1);
\draw[->] (ln1) -- (ffn);
\draw[->] (ffn) -- (add2);
\draw[->] (add2) -- (y);

% residuals
\coordinate (res1start) at ($(x.east)+(20mm,0)$);
\draw[->] (x.east) -- (res1start) node[midway,above] {\scriptsize residual};
\draw[->] (res1start) |- ($(add1.east)+(20mm,0)$) -- (add1.east);

\coordinate (res2start) at ($(add1.east)+(20.2mm,0)$);
\draw[->] (res2start) |- (add2.east);
\end{tikzpicture}

\small Pre-LayerNorm Transformer block
\end{minipage}%
\hfill
% ---------------- Post-LN ----------------
\begin{minipage}{0.45\textwidth}
\centering
\begin{tikzpicture}[>=Latex, node distance=12mm, font=\small]
\node (x)   {$\mX$};
\node (msa) [block, below=of x] {Multi-Head\\Self-Attention};
\node (add1)[plus,  below=of msa] {\small $+$};
\node (ln1) [norm,  below=of add1] {LayerNorm};
\node (ffn) [block, below=of ln1] {Feed-Forward\\Network};
\node (add2)[plus,  below=of ffn] {\small $+$};
\node (ln2) [norm,  below=of add2] {LayerNorm};
\node (y)   [below=of ln2] {$\mY$};

% main path
\draw[->] (x) -- (msa);
\draw[->] (msa) -- (add1);
\draw[->] (add1) -- (ln1);
\draw[->] (ln1) -- (ffn);
\draw[->] (ffn) -- (add2);
\draw[->] (add2) -- (ln2) -- (y);

% residuals
\coordinate (res1start) at ($(x.east)+(20mm,0)$);
\draw[->] (x.east) -- (res1start) node[midway,above] {\scriptsize residual};
\draw[->] (res1start) |- ($(add1.east)+(20mm,0)$) -- (add1.east);

\coordinate (res2start) at ($(add1.east)+(20.2mm,0)$);
\draw[->] (res2start) |- (add2.east);
\end{tikzpicture}

\small Post-LayerNorm Transformer block
\end{minipage}

\end{figure}
As we have discussed before, the Pre-LayerNorm fashion is the current state of the art as its training is more stable, as well as its convergence. 
It helps gradient flow, and deep model stability, does not requries learning rate warm up and works better with long context.
It do has downside though, Pre-LN has a weaker normalizing effect so sometimes people add an extra final LayerNorm after the last layer for output stability (``final LN'' in GPT models).

\paragraph{Decoder}
\tikzset{
  block/.style={draw, rounded corners, minimum width=30mm, minimum height=7mm, align=center},
  norm/.style={draw, rounded corners, minimum width=22mm, minimum height=7mm, align=center},
  plus/.style={circle, draw, inner sep=0pt, minimum size=4mm},
}


\begin{figure}[H]

\centering
\begin{tikzpicture}[>=Latex, node distance=12mm, font=\small]

% --- main vertical path (Pre-LN) ---
\node (x)   {$\mX$}; % decoder input (token embeddings or previous layer output)
\node (ln0) [norm,  below=of x]   {LayerNorm};
\node (msa) [block, below=of ln0] {Masked Multi-Head\\Self-Attention};
\node (add1)[plus,  below=of msa] {\small $+$};

\node (ln1) [norm,  below=of add1] {LayerNorm};
\node (csa) [block, below=of ln1] {Multi-Head\\Cross-Attention};
\node (add2)[plus,  below=of csa] {\small $+$};

\node (ln2) [norm,  below=of add2] {LayerNorm};
\node (ffn) [block, below=of ln2] {Feed-Forward\\Network};
\node (add3)[plus,  below=of ffn] {\small $+$};
\node (y)   [below=of add3] {$y$}; % decoder output

% --- encoder memory feeding cross-attention ---
% M: encoder hidden states, shape [T_enc x d_model]
\node (memtitle) [left=34mm of csa, align=left] {Encoder memory $\mM$\\ \footnotesize $\mM \in \mathbb{R}^{L_{\text{enc}}\times d_{\text{model}}}$};
% K,V label
\node (kvlabel) [below=1mm of memtitle, anchor=north west, align=left] {\footnotesize $\mK=\mW_{\mK} \mM,\;\;\mV=\mW_V \mM$};
\draw[->] (memtitle.east) -- (csa.west) node[midway,above] {\scriptsize $\mK,\mV$};

% --- main downward arrows ---
\draw[->] (x) -- (ln0) -- (msa) -- (add1) -- (ln1) -- (csa) -- (add2) -- (ln2) -- (ffn) -- (add3) -- (y);

% --- causal mask annotation into MSA ---

\node (masknote) [right=14mm of msa, align=left] {\scriptsize causal mask \\[-1pt] \scriptsize};
\draw[-{Latex}] (masknote.west) -- ++(-4mm,0) |- (msa.east);

% Optional: label MSA inputs as Q,K,V from LN(x)
\node (qkvmsa) [left=8mm of msa, align=right] {\scriptsize $\mQ,\mK,\mV$ \\ \scriptsize from $\mathrm{LN}(\mX)$};
\draw[-{Latex}] (qkvmsa.east) -- (msa.west);


% --- residuals aligned on a right column ---
\coordinate (rcol) at ($(x.east)+(23.3mm,0)$); % common right column for loops

% residual from x to add1 with explicit origin arrow
\draw[->] (x.east) -- (rcol) node[midway,above] {\scriptsize Residual};
\draw[->] (rcol) |- (add1.east);

% residual from after add1 to add2
\draw[->] ($(add1.east)+(24mm,0)$) |- (add2.east);

% residual from after add2 to add3
\draw[->] ($(add2.east)+(24mm,0)$) |- (add3.east);

\end{tikzpicture}

\caption{Pre-LN Transformer \textbf{Decoder} block. Each sublayer consumes a LayerNormed input. The masked self-attention applies a \emph{causal mask} (no future tokens). Cross-attention queries come from the decoder stream, while keys/values are derived from the encoder memory $\mM$ (encoder hidden states: $\mK=\mW_\mK\mM$, $\mV=\mW_\mV\mM$).}
\end{figure}

As a further explanation besides the figure: this $\mM$ is the matrix of encoder hidden states produced by the last encoder layer. It is what the decoder attends to in cross attention.
Let us image that an input $\mX \in \R^{B \times L_{\text{enc}} \times d_{\text{model}}}$ is fed into the encoder blocks. 
After embedding and positional encoding, we have 
\begin{align*}
  \mH^0 \in \R^{B \times L_{\text{enc}} \times d_{\text{model}}}.
\end{align*}
Each encoder will do 
\begin{align*}
  \mH^{k+1} = \text{Encoder}(\mH^k) \in \R^{B \times L_{\text{enc}} \times d_{\text{model}}}, \quad \forall k \in [N_{\text{enc}}],
\end{align*}
And $\mM$ is exactly $\mM = \mH^{N_{\text{enc}}}$.
As a result, for the cross attention layer: $\mK, \mV \in \R^{B \times L_{\text{enc}} \times d_{\text{model}}}$ and $\mQ \in \R^{B \times L_{\text{dec}} \times d_{\text{model}}}$, so the output of decoder block will always be $\R^{B \times L_{\text{dec}} \times d_{\text{model}}}$.

The two types of blocks are different:
\begin{itemize}
  \item Encoder: Self attention only, no masking, aiming to build a contextual representation of the entire source sequence.
  
  \item Decoder: Masked self attention, which prevents seeing future tokens so that the generation is autoregressive. 
  Cross attention, which lets each target position attend to the encoder's output $\mM$ (source context)
  The purpose overall is to generate the target sequence one step at a time, with access to both past target tokens and the entire source sequence.
\end{itemize}

What happens if we are using one type?
\begin{itemize}
  \item Encoder: leaking future target tokens during training (no causal mask), so the model wouldn't learn autoregressive generation.
  \item Decoder: useless masked self-attention in the encoder that blocks half the context for no reason, waste compute on a cross-attention sublayer when no encoder memory exists yet.
\end{itemize}

\paragraph{The original seq2seq transformer:} Things happens in two phases, 
\begin{verbatim}
Source tokens ---> [Encoder stack] ----------------> M (memory)
                                         |   
                                         v
Shifted target tokens ------> [Decoder stack] ---> Output logits
\end{verbatim}
Encoder runs once on the full source, decoder runs once (training) or incrementally (inference), always starting with shifted target embeddings as its own input stream.
The input of encoder is the source sequence $x = [x_1, x_2, ..., x_{L_{\text{enc}}}]$, after embedding we get $\mX$, while the input of the decoder is the target sequence $y$ so far, but shifted ($\hat{y}$) so that the model predicts the next token.
\begin{enumerate}
  \item We take the original gold target sequence $y = [y_1, y_2, ..., y_{L_{\text{dec}}}]$.
  \item Shift right by one and add the <BOS> special token, and obtain $\hat{y} = [\text{<BOS>}, y_1, ..., y_{L_{\text{dec}} - 1}]$, we then embed to get $\hat{\mY}$ ans use it as an input to the decoder blocks.
\end{enumerate}

\subsection{Encoder only models} 
BERT, RoBERTa, DeBERTa, ELECTRA, Sentence-BERT (SBERT). 

The architecture mostly are: \textbullet~ Stacks of encoder blocks only, \textbullet~ Full self attention, \textbullet~ Input sequence length stay fixed, \textbullet~ output contextualized embeddings for every token.

Purpose: Understand text: classification, regression, retrieval, token-level labeling (NER, POS tagging, QA span prediction). In those cases the model needs bidirectional context: token sees both left and right neighbors.

For many NLP tasks, we already have the full text and just need to analyze it, not generate it.

Advantages:
\begin{itemize}
  \item \textbf{Better context capture:} every token attends to all others.
  \item \textbf{More efficient training} for non-generative tasks (no need to autoregress).
  \item \textbf{Easier fine-tuning} for classification tasks: just take the [CLS] embedding\footnote{In BERT and similar models, we prepend a special token [CLS] (“classification”) to the start of every input sequence before feeding it to the encoder. [CLS] has its own trainable embedding vector in the model's vocabulary, just like any word. It is treated as position $0$ in the sequence and goes through all encoder layers along with the other tokens. After the final encoder layer, [CLS] has a contextualized vector $h \in \R^{d_{\text{model}}}$, which encodes information from the entire sequence (thanks to self-attention).}.
\end{itemize}

\subsection{Decoder only models}

GPT family (GPT-2, GPT-3, GPT-4, LLaMA, Mistral, Falcon, etc.); BLOOM, OPT, Pythia; Code generation models (CodeLLaMA, StarCoder)

Architecture: \textbullet~ Stack of decoder blocks only \textbullet~ Causal self-attention (mask future positions) \textbullet~ \textbf{No cross-attention} \textbullet~ Input length = current sequence length during generation.

Purpose: Generate text: language modeling, code generation; model learns 
\begin{align*}
  P(\text{next token} \mid \text{prev tokens})
\end{align*}

Advantages:
\begin{itemize}
  \item \textbf{Simpler}: do not need an encoder, since we aim to predict the next token given the past. Same architecture works for both pretraining (predict next token) and inference (sample next token)
  \item \textbf{Massive scalability}: can ingest any text corpus, no need for aligned parallel data.
  \item \textbf{Flexible prompts}: can condition on arbitrary text in-context.
\end{itemize}

\subsection{Encoder-decoder models}
Often on seq2seq task, requires paired data\footnote{An example is that machine translation needs paired data: Source: "I like apples", Target: "J'aime les pommes".}.


\subsection{Prefix-decoder models}
Slightly less main stream variant in the tranformer family, similar to an interpolation between decoder-only and encoder-decoder models.

\textbf{Key difference:} In prefix decoder blocks, we only have one prefix self attention layer (causal self attention layer with prefix mask), and we do not have two attentions (cross attention + self attention) pattern as in the normal decoder block.
We are essentially changing the externel information from encoder to prefix region in the mask.


Imagine now we take a sequence of length $L = m + n$
\begin{align*}
  \textbf{x} = [\underbrace{p_1, \hdots, p_m}_{\textbf{p}}, \underbrace{g_1, \hdots, g_n}_{\textbf{g}}],
\end{align*}
where $\mpp$ is the prefix tokens which are given in full before text generation starts.
This could be tokens from a source sentence (like the ``encoder output'' flattened into tokens), learnable virtual tokens (prefix tuning), encoded representations from another modality (e.g., image embeddings mapped to token space),or just a long text prompt. $\mg$ here is the generation tokens that we will generate causually one by one.

A standard decoder (decoder-only transformers) uses a strict causal mask, which is a lower triangular matrix,
\begin{align*}
  \text{Mask}[i, j] = \begin{cases}
    1 &\quad \text{ if } \quad j \leq i \\
    0 &\quad \text{ otherwise }
  \end{cases}
\end{align*}
for all $i, j \in [L]$, this says that token $i$ can only attends to positions $j \leq i$.
While for a prefix decoder, we relaxes the prefix region 
\begin{align*}
  \text{Mask}[i, j] = \begin{cases}
    1 & \quad \text{ if } \quad i \leq m \text{ and } j \leq m \qquad \text{prefix attends to all prefix} \\
    1 & \quad \text{ if } \quad i \geq m \text{ and } j \leq m \qquad \text{generation attends to all prefix} \\
    1 & \quad \text{ if } \quad i > m, m < j \leq i \qquad \text{generation attends to its own past}\\
    0 & \quad \text{ otherwise }
  \end{cases}
\end{align*}

An example is that when we want to generate a translation for a sentence $\mt$ (of tokens), we will use it as a prefix, and we know that $\mt_0$ is translated into $\ms_0$ already, in this case, our prefix would be $[\mt_0, \text{<sep>}, \ms_0, \text{<BOS>}]$ and our generation tokens would be $\mt$.
We include <BOS> in the prefix because it is a known token which is used to anchor.

We prefer prefix decoder sometimes, because 
\begin{itemize}
  \item \textbf{Fewer Parameters:} Parameter in the attention part drop roughly half.
  \item \textbf{Lower compute cost:} When prefix length is modest compared to generation length, prefix decoder can be fast overall, especially on hardware where fusing the attention into one pass matters.
  \item \textbf{Simpler architecture:} Do not need a separate encoder-decoder split
  \item \textbf{Easier caching for autoregressive generation}: In standard decoder with cross attention, we need to cache (i) decoder past key/values for masked self attention (ii) the encoder outputs for cross attention.
  In a prefix decoder, the prefix tokens are just part of the same KV-cache. 
\end{itemize}
Down sides: (1) less flexibility in terms of shared layer stack and parameters. (2) for long prefixes, attention is more expansive.


\subsection{Summary}
As a summary:
\begin{table}[H]
\label{tablex1}
\centering
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|l|p{5cm}|p{5cm}|}
\hline
\textbf{Model type} & \textbf{Pros} & \textbf{Cons} \\
\hline
\textbf{Encoder-only} & 
\textbullet~ Strong understanding with bidirectional context 

\textbullet~Great for classification, retrieval, and embedding learning
& 
\textbullet~ Not suited for generative tasks \\
\hline
\textbf{Decoder-only} &
\textbullet~ Simpler architecture 

\textbullet~ Easy to train generatively 

\textbullet~ Highly flexible for any prompt-based generation
&
\textbullet~ Lacks bidirectional context

\textbullet~ Weaker for pure understanding tasks without adaptation \\
\hline
\textbf{Encoder--decoder} &
\textbullet~ Best for sequence-to-sequence tasks (translation, summarization, speech-to-text)
&
\textbullet~ Requires paired data

\textbullet~ Heavier compute at inference (two stacks) \\
\hline
\end{tabular}
\caption{Comparison of encoder-only, decoder-only, and encoder--decoder Transformer architectures.}
\end{table}


\subsection{The Linear Layers}
FFNs (Feed Forward Networks) are a part of the encoder, decoder block, there are several design choices here.
They are also known as MLPs (Multi-Layer Perceptrons), and they are used to transform the input features into a higher-dimensional space, apply non-linear activation, and then project back to the original dimension.
For the 1-$d$ case, imagine we have a vector $x \in \R^{d_{\text{model}}}$, then the FFN does the following:
\begin{align*}
  \text{FFN}(x) = \mW_2\cdot\sigma(\mW_1 x + b_1) + b_2,
\end{align*}
where $mW_1 \in \R^{d_{\text{ffn}} \times d_{\text{model}}}$ is the matrix that projects the input to a higher dimension, $\sigma$ is a non-linear activation function (e.g., ReLU, GELU), and $\mW_2 \in \R^{d_{\text{model}} \times d_{\text{ffn}}}$ projects it back to the original dimension.
$d_{\text{ffn}}$ is often $2 ~ 4 \times$ $d_{\text{model}}$.
Now if we consider the true input $\mX \in \R^{B \times L \times d_{\text{model}}}$, then basically, for each sequence in the batch, we apply the same FFN to each token, and there will be no interactions between different tokens.

\paragraph{Why do we put FFNs in the encoder/decoder blocks?}
The FFN is used to introduce non-linearity into the model, allowing it to learn more complex representations. 
Self-attention is linear in the feature dimension for a fixed set of attention weights.
It mixes tokens but does not increase the per-token expressivity much.
To see this, consider the following attention formula:
\begin{align*}
  \text{Attn}(\mQ, \mK, \mV) = \underbrace{\text{softmax}\left(\frac{\mQ\mK^\top}{\sqrt{d_{\text{model}}}}\right)}_{\eqdef \mA}\mV,
\end{align*}
if the attention weight matrix $\mA$ is fixed, then the output is a linear combination of the input $\mV$ which is linear in $\mX$, i.e, linear in the feature dimension.

If we \textbf{remove the FFN}, the model would reduce to mostly linear mixing layers accross tokens, which leads to collapse of expressivity (model underfits complex transformations), and much worse performance.


\paragraph{Design choices for FFNs:}
There are several design choices for FFNs, which can affect the model's performance and efficiency.
\begin{itemize}
  \item Expansion ratio: $d_{\text{ffn}}/d_{\text{model}}$.
\end{itemize}
This is typically around $2 ~ 4 \times$.
In the original Transformer paper, it is $4 \times$, while in LLaMA, it is $2 \times$.
In many large models, FFNs account for $50\% - 60\%$ of total parameters.

\begin{itemize}
  \item Activation function: ReLU, GELU, SiLU, etc.
\end{itemize}
The original Transformer paper uses ReLU, but GELU is more popular in modern models (BERT, GPT-2/3).

ReLU: piecewise linear, fast, but can lead to dead neurons (zero gradients for negative inputs). 
\begin{align*}
  \text{ReLU}(x) = \max(0, x).
\end{align*}

GELU: a smoother, probabilistic activation function that approximates the Gaussian distribution.
\begin{align*}
  \text{GELU}(x) = x \cdot \Phi(x) = x \cdot \frac{1}{2}\left(1 + \text{erf}\left(\frac{x}{\sqrt{2}}\right)\right),
\end{align*}
where $\Phi(x)$ is the cumulative distribution function of the standard normal distribution, and $\text{erf}$ is the error function.
\begin{itemize}
  \item Pros: \textbf{Smooth, i.e., differentiable everywhere}\footnote{Smoothness in this context refers to infinitely differentiable functions $C^{\infty}$, not the smoothness in the optimization theory sense.}, which enables better gradient flows. It often yields better performance in transformers, understanding and generation tasks.
  \item Cons: \textbf{Computationally more expensive}, sligltly more than ReLU.
\end{itemize}

SiLU: another smooth activation function, models such as EfficientNet used it. 
\begin{align*}
  \text{SiLU}(x) = x \cdot \sigma(x),
\end{align*}
where $\sigma(x) = \frac{1}{1 + e^{-x}}$ is the sigmoid function.
It is sometimes called the Swish-1 (Swish with $\beta = 1$.)


\subsection{Gated Linear Units}
GLU (Gated Linear Unit) takes an input, splits it into two parts (value \& gate), and uses one to gate the other via an elementwise product.
Mathematically, for $\mX \in \R^{B \times L \times d_{\text{model}}}$, we first apply two independent linear projections using $\mW_g, \mW_v \in \R^{d_{\text{ffn}} \times d_{\text{model}}}$, 
\begin{align*}
  \mG = \mW_g \mX + \mB_g, \qquad \mV = \mW_v \mX + \mB_v;
\end{align*}
We then apply a nonlinear activation to the gate part, after which we do elementwise multiplication.
\begin{align*}
  \mG^\prime = \sigma(\mG), \qquad \text{GLU}(\mX) = \mV \odot \mG^\prime.
\end{align*}

Note that we are applying activation to $\mG$ (the gate) instead of $\mV$, because we are applying the non-linearity here to shape its gating behaviour.
This separation between gate and value allows us to learn specialized gating pattern, which acts like an learned feature selector.

\paragraph{SwiGLU:} we are essentially replacing the original activation function $\sigma(\cdot)$ with $\text{SiLU}(\cdot)$, which is smoother. 
This is used in LLaMA, PaLM, etc.

\paragraph{GEGLU:} we are essentially replacing the original activation with $\text{GELU}(\cdot)$ or its approximations with tanh.


\subsection{Mixture-of-Experts FFNs}
In a standard Transformer block, the FFN is a single feed-forward network (sometimes gated like SwiGLU) applied to every token. 
In MoE, instead of one FFN, we have $E$ separate FFN ``experts''. 
For each token, a router chooses a subset (often 1 or 2) of experts to run.
The outputs are combined according to the router's weights.
Its advantages include 
\begin{itemize}
  \item \textbf{Scaling parameters without scaling compute:} We can have lots of experts trained but we only invoke one of them. 
  \item \textbf{Specialization:} The model specializes experts for different types of inputs (topics, syntactic structures, etc.).
\end{itemize}

Mathematically, let $\mX \in \R^{B \times L \times d_{\text{model}}}$, $E$ be the number of experts in this case. 

The forward pass:

\begin{itemize}
  \item[(i)] Router logits:
  \begin{align*}
    \mR = \mW_r \mX + \mB_r \quad \in \RBLE,
  \end{align*}
  where $\mB_r = \mathbf{1}b_r^{\top}$ is the extended bias matrix. 
  \item[(ii)] Gating probabilities:
  \begin{align*}
    \mP = \softmax (\mR) \quad \in \RBLE.
  \end{align*}

  \item[(iii)] Top-$k$ routing: 
  Keep the best-$k$ experts per token according to the probabilities, and renormalize.
  Specifically, for each token $l$ in each batch $b$, denote $\cT_k(b, l)$ as the indices of top-$k$ experts according to $\mP[b, l, :]$, we do 
  \begin{align*}
    \tilde{\mP}[b, l, d] = \frac{\mP[b, l, d]}{\sum_{j=1}^{\cT_k(b, l)}\mP[b, l, j]},\quad \forall d \in \cT_k(b, l), \quad \lvert \cT_k(b, l) \rvert = k,
  \end{align*} 
  and sets the other coordinates to be zero, which allows us to obtain $\tilde{\mP}$.
  The renormalization here is to avoid suppressing magnitude purely because we dropped low-prob experts.

  \item[(iv)] Dispatch:
  For each expert $e$, define the packed input by selecting tokens routed to $e$ as $\mX_e$.
  Let $N_e$ be the number of tokens whose top-$k$ includes expert $e$, $\pi_e(\cdot)$ be the corresponding map of expert $e$ (from $\cbrac{1, \dots, N_e} \mapsto \cbrac{[b, l]}$) to the original indices pair $(b, l)$ and let $\eta_e(\cdot)$ be the map from $\cbrac{[b, l]}$ to the index in $[N_e]$.
  Then the input for an expert is 
  \begin{align*}
    \mX_e \in \R^{N_e \times \dmodel}, \quad \text{    where    }\quad \mX_e[j, :] = \mX[\pi_e(j), :], \qquad \forall j \in [N_e]
  \end{align*}
  with the renormalized gate weight 
  \begin{align*}
    g_e \in \R^{N_e}, \quad \text{    where    }\quad g_e[j] = \tilde{\mP}[\pi_e(j), e].
  \end{align*}

  \item[(v)] Expert maps:
  Depending on the expert type (dense FFN / Gated FFN), we create 
  \begin{align*}
    \mY_e = \text{FFN}_e(\mX_e) \quad \in \R^{N_e \times \dmodel}.
  \end{align*}

  \item[(vi)] Gate scaling and combine:
  Apply per token gate (weight) to the output 
  \begin{align*}
    \tilde{\mY}_e = \diag (g_e) \cdot \mY_e, \qquad (\text{in ML literature} \qquad g_e \odot \mY_e \text{ using broadcast.}) 
  \end{align*}
  Then the output $\mZ \in \RBLDm$ would be filled such that 
  \begin{align*}
    \mZ[b, l, :] = \sum_{e \in \cT_k(b, l)} \tilde{\mY}_e[\eta_e(b, l), :]
  \end{align*}
  before outputing.
\end{itemize}

\paragraph{Capacity constraint:} In practical implementation, we often set \textbf{capacity constraint} in its formalization, which is the limit on how many tokens each expert is allowed to process in a single forward pass.
The reason to have it is 
\begin{itemize}
  \item \textbf{Load imbalance is common:} one ``hot'' expert could get far more tokens than the average, causing GPU memory overflow for that expert, and slower steps since we need to wait for it.
  \item \textbf{Hardware needs fixed allocation:} In distributed training, each expert lives on one or more GPUs, memory buffers for expert input/output must be pre-allocated. 
  If the number of tokens per expert varies wildly, we cannot pre-allocate efficiently without wasting huge amounts of memory.
\end{itemize}
We set a capacity per expert:
\begin{align*}
  C_e = \floor{\alpha \cdot \frac{BL}{E}},
\end{align*}
where $E$ is the number of experts and $\alpha \geq 1$ is the capacity factor to allow slackness. 
If an expert get more than $C_e$ tokens:
\begin{enumerate}
  \item \textbf{Dropping:} (common in Switch Transformers) Excess tokens are simply dropped for that expert: they don't get processed there, and their contribution from that expert is zeroed.
  \item \textbf{Rerouting} (less common) Excess tokens are sent to a backup expert.
\end{enumerate}
If an expert get fewer than $C_e$ tokens: one may use \textbf{padding} we pad its buffer to $C_e$ entries for vectorization but those padded entris are ignored.


\paragraph{Load balancing auxiliary loss:} 
We need an extra loss term during training so that the router doesn't collapse onto just a few experts. 
The reasion is that, without any extra incentive, the model may prefer some experts a lot more than others, leaving many experts underused or unused.
We have 
\begin{align*}
  \cL_{\text{total}} = \cL_{\text{task}} + \lambda \cdot \cL_{\text{aux}},
\end{align*}
where $\cL_{\text{task}}$ is the task loss, and in the common GShard / Switch Transfomer\footnote{Both of these are specific MoE implementations from Google that popularized large-scale sparse FFNs. Switch is basically a simplified, more efficient GShard.}, we have 
\begin{align*}
  \cL_{\text{aux}} = E \cdot \sum_{e=1}^{E} f_e \cdot m_e.
\end{align*}
$f_e$ is defined as the fraction of tokens assigned to expert $e$,
\begin{align*}
  f_e = \frac{N_e}{BL} = \frac{1}{BL}\sum_{b=1}^{B}\sum_{l=1}^{L}\mathbf{1}\cbrac{e \in \cT_k(b, l)},
\end{align*}
where $\cbrac{e \in \cT_k(b, l)}$ is the indicator vector (sum of corresponding one hot vector) in $\R^{B \times L}$ suggesting if expert $e$ is selected by this token, while $m_e$ is defined as 
\begin{align*}
  m_e = \frac{1}{BL} \sum_{b=1}^{B} \sum_{l=1}^{L} P[b, l, e],
\end{align*}
which is the average probability of assigning to expert $e$.
One can show that the loss is smaller when $f_e$ and $m_e$ are more evenly distributed across experts. 
$\cL_{\text{aux}}$ can be though of as an load balancing regularizer.

\paragraph{The all to all trick:}
In distributed training, experts are sharded across devices (e.g., $1$ expert per GPU or multiple experts per GPU.)
Naively, we must 
\begin{enumerate}
  \item Send tokens to the device hosting their assigned expert.
  \item Process them locally in the expert's feed-forward network.
  \item Send back the processed outputs to the original device to continue the model pipeline.
\end{enumerate}
This is a typical many-to-many communication patter, where  every GPU may need to send tokens to every other GPU.
The most straight forward way is to let all tokens sent to all devices before filtering using a series of all\_gather operations, but this can waste bandwitdh.

We can do it with the \textbf{all to all} trick, which use a collective communication primitive where each GPU directly sends only the tokens that the other GPUs need, in one coordinated call.
In NCCL and similar libraries
\begin{verbatim}
  torch.distributed.all_to_all_single(output, input, ...)
\end{verbatim}

In practice, MoE frameworks do two all-to-all per MoE layer:
\begin{enumerate}
  \item \textbf{Dispatch:} tokens → owning expert GPU.
  \item \textbf{Combine:} expert outputs → original token order.
\end{enumerate}
In this way, redundant communication is reduced.
Real world examples includes GShard / Switch Transformer / DeepSpeed-MoE / Megatron-MoE.

\paragraph{Processes and threads:} The legacy $\boxed{\text{nn.DataParallel}}$ (\textbf{DP}) is based on one process multi-threading, which is generally slower and less scalable (single optimizer state, host-side bottlenecks, GIL contention, extra device = host hops).
This is why \textbf{DDP} (multiprocess, one rank per GPU) is recommended.

Reasons:
\begin{enumerate}
  \item \textbf{CUDA context is per-process:} Each process gets a clean, single-GPU context, which leads to simpler, safer memory management and fewer heisenbugs.
  \item \textbf{Communication stacks (NCCL)} are designed around ranks = processes.
  \item \textbf{Python GIL:} Python threads can't execute Python bytecode truly in parallel. Many CUDA ops release the GIL(Global Interpreter Lock), but coordination/launch logic still contends. One process per GPU avoids that bottleneck.
  \item \textbf{Failure isolation:}  If one GPU OOMs or a kernel errors, you don't take down every replica in the same process.
  \item \textbf{Deterministic performance:} Independent schedulers per process avoid thread scheduling contention and reduce cross-device interference within a single interpreter.
\end{enumerate}



\subsection{Parallel Attention + FFN}
The parallel design changes the flow so that attention and FFN happen at the same time on the same input (after normalization), instead of one after the other.
Their outputs are summed together with the residual connection in a single step.
\begin{verbatim}
          ↑      → FFns      →     ↓
   x → LayerNorm → Attention → Summation
   ↓             → Residual  →     ↑
\end{verbatim}
Why do we do this:
\begin{itemize}
  \item \textbf{Parallelism:} Attention and FFN can be computed concurrently.
  \item \textbf{Fewer LayerNorms:} Saves computation.
  \item \textbf{Information Flow:} In sequential design, the FFN only sees the post-attention representation. In parallel, FFN sees the original representation directly, possibly preserving more raw token information.
  \item \textbf{Better gradient flow:} Shorter path between input and output in the computation graph.
\end{itemize}
This architecture is used in some newer LLMs, e.g. PaLM and GPT-JT, to improve speed without hurting performance.

Some practical notes:

\textbullet~ \textbf{Scaling:} Some designs scale the attention and FFN outputs to balance their contributions.
  
\textbullet~ \textbf{DropPath / Stochastic depth:} In residual networks, instead of always computing the residual branch, we may randomly drop it during training according to some probabilities, so that the model learns to rely on multiple paths, not just one.
In parallel attention, we may consider dropping attention branch and FFN branch independently. 
In this way we reduces overfitting by introducing randomness, and encourages robustness.

\textbullet~ \textbf{Variance stability:} When we sum two residual branches instead of one, we risk blowing up the variance of activations early in training. 
This is another reason why we do scaling:
\begin{align*}
  x^\prime = x + \frac{1}{\sqrt{2}}\rbrac{\text{attn\_out} + \text{ffn\_out}}
\end{align*}
We can also control by careful weight initializations, or considering set a learnable scaling parameters like $\alpha_{\text{attn}}$ and $\alpha_{\text{ffn}}$ which is small intially and train up.






\section{Computing the Number of Parameters}


\section{Fintuning}

\subsection{Pretraining Loss (self-supervised)}
As we know, in pretraining, we are basically doing a self-supervised learning task on predicting the next token.
Given an sequence $\mx = (\mx_1, \hdots, \mx_L)$, we are trying to model 
\begin{align*}
  \probsub{\theta}{\mx_1, \hdots, \mx_L} = \prod_{t=1}^L \probsub{\theta}{\mx_t \mid \mx_{< t}},
\end{align*}
where $\theta$ represnets all the trainable parameters of the language model. 
The standard choice for it is the causal language modeling (CLM) loss:
\begin{align}
  \label{eq:CLM}
  \cL_{\text{pretrain}} \rbrac{\theta} = -\frac{1}{L}\sum_{t=1}^{L}\log\rbrac{\probsub{\theta}{\mx_t \mid \mx_{<t}}}, \tag{CLM}
\end{align}
which is the negative log-likelihood over the sequence.
This can be written equivalently as the cross entropy between the model's predicted distribution $\probsub{\theta}{\cdot \mid x_{< t}} \in \R^{\lvert\cV \rvert}$ and the one-hot true token $\my_t \in \cbrac{0, 1}^{\lvert\cV \rvert}$, where $\cV$ is the vocabulary,
\begin{align}
  \label{eq:CrossEntropy}
  \cL_{\text{pretrain}}\rbrac{\theta} = -\frac{1}{L}\sum_{t=1}^{L}\sum_{v \in \cV} \rbrac{\mx^\prime_{t}}_v \log \probsub{\theta}{v \mid \mx_{<T}}, \tag{Cross Entropy}
\end{align} 
where $\rbrac{\mx^\prime_t}_v = \mathbf{1}\cbrac{v = \mx_t} \in \R^{\abs{\cV}}$.
Basically, we are summing over all possible vocabularies.
The equivalence is clear once we see that, in \eqref{eq:CrossEntropy} $\mx^\prime_t$ itself is a one hot vector, and it is $1$ nowhere but when $v = \mx_t$, with everywhere else $0$.
The training itself reduces to a \textbf{maximum likelihood} problem given a trainable set of parameter $\theta$.  
A quick implementation is that 
\begin{verbatim}
  loss = F.cross_entropy(
      logits[:, :-1, :].reshape(-1,  vocab_size), # (B, L, V) -> (BL, V)
      tokens[:, 1:].reshape(-1) # (BL, V)
    )
\end{verbatim}



\subsection{Finetuning Loss}
When adapting to downstream tasks or human preferences, loss functions differ.
Typical approaches include Supervised Fine-Tuning (SFT), Parameter-Efficient Fine-Tuning (PEFT) (such as LoRA and QLoRA), Reinforcement Learning from Human Feedback (RLHF)\footnote{Reinforcement learning with a reward model.}, DPO / IPO, continual pretraining, domain adaptation.


\subsection{Supervised Fine-Tuning (SFT)}
Our goal is to make the LLM output exactly match human-provided target completions given prompts.
Our loss is still cross-entropy loss, but computed \textbf{only} over the target part of the sequence.

Given a prompt $\mpp$ of length $L_{\mpp}$ and a target $\my$ of length $L_{\my}$, we compute 
\begin{align}
  \cL_{\text{SFT}} = -\frac{1}{L_{\my}} \sum_{t=1}^{L_{\my}} \log \rbrac{\probsub{\theta}{\my_t \mid \my_{< t}, \mpp}} = \frac{1}{L_{\my}}\sum_{t=1}^{L_{\my}}\sum_{v \in \cV}\rbrac{\my^\prime_t}_v\log\rbrac{\probsub{\theta}{v \mid \mpp, \my_{< t}}}, \tag{SFT}
\end{align}
where $\rbrac{\my^\prime_t}_v = \mathbf{1}\cbrac{v = \my_t} \in \R^{\abs{\cV}}$.
Notice that only completetion tokens are included in the sum (masking the prompt tokens in loss calculation).
In practice we will just mask in the following way.
\begin{align*}
  \underbrace{\text{<bos>  } \text{prompt\_tokens}}_{\text{Mask}} \text{ target\_tokens } \text{<eos>}. 
\end{align*}




\subsection{Parameter-Efficient Fine-Tuning (PEFT)}  Updating all parameters of a model is costly. 
To resolve this, one way is to use the so called ``adaptors''.
Other approaches include LoRA, QLoRA.

\paragraph{Adaptors} In parameter-efficient fine-tuning (PEFT), adapters are small trainable modules inserted into a frozen pre-trained model's layers, typically after the feed-forward (FFN) or attention sublayer.
The core idea is that, 

\textbullet~ We freeze the original model weights.

\textbullet~ Insert a lightweight bottleneck layer (down-projection → non-linearity → up-projection) inside each transformer block.

\textbullet~ Train only these new parameters,

Mathematically, for an ouput $\mh \in \R^{\dmodel}$ (we temporarily skip the batch and sequence length dimension as we are doing this to each token's represenetation.)
\begin{align*}
  \text{Adapter}(\mh) = \mh + \mW_{\text{up}} \cdot \sigma\rbrac{\mW_{\text{down}}\mh}, 
\end{align*}
We often have $\mW_{\text{down}} \in \R^{d_{\text{down}} \times \dmodel}$ where $d_{\text{down}} << \dmodel$ so that the total number of trainable parameters are drastically reduced ($<5\%$ of the full model).


\paragraph{Low-Rank Adaptation of LLMs} Our goal here remains the same, we are trying to fine-tune large models with far fewer trainable parameters by only learning a low-rank update to certain weight matrices.

\textbullet~ \textbf{LoRA}

Formally, consider a weight matrix $\mW_0 \in \R^{\dout \times \din}$ in the pre-trained model, in stead of update its pretrained parameter $\mW_0$ directly, we keep it frozen, and learn: 
\begin{align*}
  \mW = \mW_0 + \Delta \mW, \qquad \Delta \mW = \frac{\alpha}{r}\mB\mA,
\end{align*}
where $\mA \in \R^{r \times \din}$, $\mB \in \R^{\dout \times r}$, $r$ is the low rank parameter, $\alpha$ is a scaling factor.

\begin{enumerate}
  \item Usually, LoRA is applied to attention projection matrices ($\mW_q$, $\mW_v$ and sometimes $\mW_k$, $\mW_o$).
  \item Rank $r$ and scaling $\alpha$ are hyperparams.
\end{enumerate}

Its advantage includes: 
\begin{enumerate}
  \item Drastic parameters savings.
  \item No added inference latency if merged into $\mW_0$ after training.
\end{enumerate}

Notice that empirically, touching $\mW_q$ and $\mW_v$ is basically enough since it recovers almost all the performance of full fine-tuning, while touching fewer parameters.
Intuitively, $\mW_q$ and $\mW_k$ affactes the matching space of attention.
Since $\mQ\mK^{\top}$ is bilinear, it often suffices to change $\mW_q$.  
$\mW_v$ controls the attented values and changing it also affects the post projection controlled by $\mW_o$.

There is a reason why do we apply a scaling factor here:
\begin{enumerate}
  \item \textbf{Training stability:} If $\mA$, $\mB$ are initialized with small random values, the magnitude of their product varies with $r$, and without scaling, high $r$ can produce much larger updates, destabilizing training. 
  \item \textbf{rank normalization} This keeps the expected variance of the update constant across different $r$ values. 
  $\alpha$ then allows manual controls (dependent on sensitivity).
\end{enumerate}
Empirically $\alpha = r$ or $\alpha = 1$.

The parameter $r$ controls the capacity of the low rank update.
Small $r$ means fewer trainable parameters and lower expressive power, works fine for minor domain shifts.
Large $r$ means more trainable parameters and expressive power, but if it is too large, then there is a risk of overfitting especially if training data is small. 
Empirically, $r = 4$ to $16$ common for LLaMA, GPT-J, BLOOM, etc.
Higher $r$ used in complex multi-task fine-tuning.
Futher improving $r$ often gives negligible benefits.


\textbullet \textbf{QLoRA}
This actually refers to Quantized LoRA, which allows fine-tuning very large language models (e.g., 65B params) on a single GPU without running out of memory.
It combines $4$-bit quantization for the frozen base model and LoRA.

\begin{enumerate}
  \item Quantize $\mW_0$ to $4$-bit NormalFloat (NF4)\footnote{It is a $4$-bit quantization scheme that preserves more precision than plain int4 by using a learned normal distribution mapping.}, which cuts memory usage by $4$.
  \item Keep $\mW_0$ frozen in quantized form.
  \item Train LoRA modules $\mA$, $\mB$ in FP16 / bfloat16.
  \item During forward pass: \begin{itemize}
    \item Dequantize $\mW_0$ on the fly.
    \item Add $\Delta \mW$ from LoRA, and proceed as usual.
  \end{itemize}
\end{enumerate}

Note that there are double quantization effective in QLoRA, 
\begin{enumerate}
  \item \textbf{First quantization:} The original pretrained weight tensor $\mW_0$ is quantized is quantized from FP16/BF16 to $4$-bit NF4 values plus per-block scaling factors (distribution-aware mapping, so it preserves more accuracy than uniform int4).
  \begin{align*}
    \mW_0 = s \cdot \mW_0^{\text{quantized}}. 
  \end{align*}
  
  \item \textbf{Second quantization:} The scaling factors $s$ themselves are quantized into a lower-precision format (8-bit) plus a meta-scaling factor.
\end{enumerate}



\subsection{Reinforcement Learning from Human Feedback (RLHF)}
The goal is to train (finetune) a language model so its outputs align with human preferences by using reinforcement learning where the “reward” comes from human judgments.

Mathematically, let us first define the concept of the policy.
Given $\mx$ as an input sequence (prompt, context) and $\my$ of length $T$ as an output sequence (continuation, response), let $\pi_0(\my \mid \mx)$ be defined as the probability that he model generates $\my$ given $\mx$, defined token-by-token:
\begin{align*}
  \pi_0(\my \mid \mx) = \prod_{t=1}^T \pi_0(\my_t \mid \mx, \my_{<t}).
\end{align*}
We are using $\pi$ as our notation for it is standard in reinforcement learning\footnote{In RL, statues $s_t$ is current context (prompt + generated tokens so far), action $a_t$ is next token to generate, policy $\pi(a_t \mid s_t)$ gives the probability distribution. The pretrained LM is thus an autoregressive policy that maps states (text histories) to action distributions (token probabilities).}.

\paragraph{Stage 1: Supervised Finetuning} We prepare a supervised dataset $\cD_{\text{SFT}} = \cbrac{(\mx_i, \my_i^\star)}_{i=1}^N$ where $\mx_i$ is prompt tokens, $\my_i$ is gold-standard human-written completion (target tokens). 
Typically, the dataset size is $10^3 - 10^6$, in this stage we do standard SFT trying to minimize the cross-entropy loss on the target tokens, which can be equivalently formulated as 
\begin{align*}
  \theta_{\text{SFT}} = \arg\min_{\theta} \ExpSub{(\mx, \my^\star \sim \cD_{\text{SFT}})}{- \log \pi_{\theta}(\my^\star \mid \mx)},
\end{align*}
where 
\begin{align*}
  \log \pi_{\theta}(\my^\star \mid \mx) = \sum_{t=1}^{L_{\my^\star}}\log \pi_{\theta}\rbrac{\rbrac{\my^\star}_t \mid \mx,  \rbrac{\my^\star}_{<t}}.
\end{align*}
Basically\footnote{Here we use $\rbrac{\cdot}_t$ to denote the $t$-the component of the vector.}, we are only computing the cross entropy loss on the human labeled completions (target sequences) without touching the prompt.


\paragraph{Stage 2: Reward Modelling} We prepare a preference dataset $\cD_{\text{pref}} = \cbrac{(\mx_j, \my_j^+, \my_j^-)}_{j=1}^M$ where $\mx$ is the prompt, $\my_j^+$ is the preferred output and $\my_j^-$ is the less preferred output, $M$ is the total number of pairs of preference pairs ($10^4$ to $10^6$) coming from human labelers ranking multiple model completions for the same prompt.


\textbf{Padding:} We define the reward model $R_{\phi}(\mx, \my)$ whose input is the (input prompt tokens, completion), i.e. $(\mx, \my)$ and outputs a scalar reward score.
Typically, we start with a base language model architecture (often the same type as the policy, e.g., Transformer decoder), and we rplace the usual LM head (which produces a distribution over next tokens) with a scalar regression head. 
Without the language modeling head, the output is a hidden state for each $(\mx, \my)$
\begin{align*}
  \mh = \rbrac{\mx, \my}0 \in \R^{B \times (L_{\mx} + L_{\my}) \times \dmodel}
\end{align*}
One may notice that for each pair of $\mx_i, \my_i$, the length $L_{\mx_i} + L_{\my_i}$ is not the same, so we must do padding in order to make it a batch \textbf{before} inputting them to the model.
We may pad all sequences on the left/right to ensure that they are the same length $L_{\max}$ to form a batch (not the global maximum, otherwise too wasteful)\footnote{This is called dynamic padding and is standard in Hugging Face DataCollatorWithPadding}, and record a corresponding attention mask $\mm_i \in \cbrac{0, 1}^{B \times L_{\max}}$ ($1$ for real token).
Often the paddings are treated as normal tokens and its embedding is either a learned vector or all zeros. 

\textbf{Batch Forming:} Now for notational convenience, let us define 
\begin{align*}
  \ms^+ = (\mx, \my^+) \in \N^{L_{\mx} + L_{\my^+}} \quad \text{ and } \quad \ms^- = (\mx, \my^-) \in \N^{L_{\mx} + L_{\my^-}},
\end{align*}
we padded them locally to form a batch $\mS = \cbrac{\hat{\ms}_1^+, {\ms}_1^-, \hdots {\ms}_B^+, {\ms}_B^-}$ where each element is now $L_{\max}$ long, with its attention mask $\mM \in \cbrac{0, 1}^{2B \times L_{\max}}$.
We feed them to the headless model and obtain 
\begin{align*}
  \mH \in \R^{2B \times L_{\max} \times \dmodel}.
\end{align*}

\textbf{Pooling:} We now need to do pooling to choose a single vector for the entire sequence to be used for pairwise comparison. 
We can choose 

\textbullet~ (A) Last-token pooling (most common), which chooses the representation of the last non-PAD tokens (since it contains context from all preceding tokens because of casual attention). 

\textbullet~ (B) Mean pooling over completions (ignore prompt tokens if we only want to summarize the generated output).

\textbullet~ (C) Special token pooling (Insert a <EOS> token and take its hidden state), works as [CLS] pooling in BERT.

After those steps, $\mH$ becomes $\mP \in \R^{2B \times \dmodel}$.

\textbf{Scalar Reward:} After we obtain $\mP$, we apply a reward head to it, which forms the reward 
\begin{align*}
  R_{\phi}(\mx, \my) = \mW_r \mP + \mb_r \in \R^{2B},
\end{align*}
where we get a scalar value for each sequence.

\textbf{Computing Loss:} We appear the pairwise Bradley-Terry loss: 
\begin{align*}
  \cL_{\text{BT}}(\phi) = -\frac{1}{B}\sum_{i=1}^{B}\log \sigma\rbrac{R_{\phi}\rbrac{\mx_i, \my_i^+} - \R_\phi\rbrac{\mx_i, \my_i^-}},
\end{align*}
The above loss encourages $R_{\phi}\rbrac{\mx_i, \my_i^+}$ to be much larger than $R_{\phi}\rbrac{\mx_i, \my_i^-}$.

This is not the only way we can choose this loss, we can also pick hinge loss:
\begin{align*}
  \cL_{\text{Hinge}}(\phi) = \frac{1}{B}\sum_{i=1}^{B} \max\cbrac{0, m - \rbrac{R_{\phi}\rbrac{\mx_i, \my_i^+} - \R_\phi\rbrac{\mx_i, \my_i^-}}},
\end{align*}
where we tries to enforce a margin $m > 0$ and if that is satified, no gradient will be applied.
Compared to BT-loss, it is non-smooth.

Mean-squared error loss can also be used:
\begin{align*}
  \cL_{\text{MSE}}(\phi) = \frac{1}{B}\sum_{i=1}^{B}\rbrac{R_{\phi}\rbrac{\mx_i, \my_i^+} - \R_\phi\rbrac{\mx_i, \my_i^-} - t}^2,
\end{align*}
where $t$ is a target gap which assumed to be known (which is also the reason why it is used).

If we have more than $2$ candidates ranked, we can normalize the exponentiated rewards into a softmax, say we have $\my^1, \my^2, \hdots, \my^K$ 
\begin{align*}
  P_{\phi}(\my^j \mid \mx) = \frac{\exp\rbrac{R_{\phi}\rbrac{\mx, \my^j}}}{\sum_{j=1}^{K}\exp\rbrac{R_{\phi}\rbrac{\mx, \my^j}}},
\end{align*}
then minimize the cross entropy loss based on this,
\begin{align*}
  \cL (\phi) = - \sum_{i=1}^M\sum_{j=1}^{K} \text{prob\_of\_rank}_j \cdot \log \rbrac{P_{\phi}(\my^j \mid \mx)}, 
\end{align*}
where this prob\_of\_rank can be $1$ for the top choice and $0$ for others, or fractional.

Note that the starting point of $\phi$ is the pretrained headless model.
We can choose to use the version with/without SFT, most pipelines use the one with SFT.
\begin{enumerate}
  \item Pros 1: RM is specialized to the distribution of completions the RL stage will actually produce
  \item Pros 2: KL penalty in PPO is between two models (policy and reference) that are both in-domain with respect to RM's training data
  \item Cons 1: RM might overfit to the stylistic quirks of the SFT model and undergeneralize to very different policies
\end{enumerate}



\paragraph{Stage 3: RL fine-tuning with PPO (Proximal Policy Optimization)} 
We have the reference policy after SFT, which we denote as $\pi_{\text{SFT}} = \pi_{\text{ref}}$ and the reward model $\R_{\phi}$.
Assume that now we have a prompt set $\cD_{\text{RL}}$ which are unlabeled instructions / prompt drawn from a prompt distribution (can be the same pool used in SFT, plus extra domains).

\textbf{RL objective:} We intialize our policy as $\pi_{\theta} = \pi_{\text{ref}}$, we want to now update $\pi_{\theta}$ so that it scores high under $R_{\phi}$ and stay close to $\pi_{\text{ref}}$ (to avoid drifting into low-quality or unsafe behavior), which can be described by the following KL-regularized objective:
\begin{align}
  \label{eq-obj-RL}
  \max_{\theta} \ExpSub{\substack{\mx \sim \cD_{\text{RL}} \\ \my \sim \pi_{\theta}(\cdot \mid \mx)}}{R_{\phi}(\mx, \,\my) - \beta \cdot {\rm D}_{\text{KL}}\rbrac{\pi_{\theta}(\cdot \mid \mx) \parallel \pi_{\text{ref}}(\cdot \mid x)}},
\end{align}
where $\beta > 0$ is the KL-coeffficient\footnote{Note that this is two nested expectation, first we can sample $\mx$ from the dataset, then because that the policy it self is stochastic, we have multiple possible $y$.}.
Notice that to compute the sequence level KL divergence as given above, we may decompose it into token level contribution.
For one sequence $\my = \rbrac{\my_1, \hdots, \my_T}$ generated from prompt $\mx$,
\begin{align*}
  \klD{\pisub{\theta}{\cdot}{\mx}}{\pisub{\text{ref}}{\cdot}{\mx}} = \ExpSub{\my \sim \pisub{\theta}{\cdot}{\mx}}{\sum_{t=1}^{T}\log \frac{\pisub{\theta}{\my_t}{\mx, \my_{<t}}}{\pisub{\text{ref}}{\my_t}{\mx, \my_{<t}}}},
\end{align*}
we can see that the token level contribution is given by 
\begin{align*}
  \text{KL}_t \rbrac{\mx, \my_{<t}, \my} = \log \frac{\pisub{\theta}{\my_t}{\mx, \my_{<t}}}{\pisub{\text{ref}}{\my_t}{\mx, \my_{<t}}}.
\end{align*}
If we think about it, this discourages the case that the probability of policy $\pisub{\theta}{\cdot}{\mx, \my_{<t}}$ generates $\my_t$ is smaller than that of policy $\pisub{\text{ref}}{\cdot}{\mx, \my_{<t}}$.

\paragraph{Sampling:} 
We can apply certain \textbf{sampling strategies} for turning the model's per-token probability distribution into actual token choices during rollout.

\textbullet~ \textbf{Temperature sampling:} assume the size of the vocabulary is $V$, for logits $\mz_t \in \R^{V}$ generated by the model at the current step $t$, we have the softmax probability 
\begin{align*}
  \mpp_i = \frac{\exp(\mz_i)}{\sum_{j=1}^{V}\exp(\mz_j)},
\end{align*} 
however, we can add another parameter which is the so called temperature $T > 0$ and rescales the logits before softmax 
\begin{align*}
  \mpp_i^{(T)} = \frac{\exp(\mz_i / T)}{\sum_{j=1}^{V}\exp(\mz_j / T)}.
\end{align*}
Observation:
\begin{enumerate}
  \item $T=1$: normal sampling from model's predicted distribution.
  \item $T>1$: flatter, more uniform, more randomness.
  \item $T<1$: sharper, more peaked, less randomness, more deterministic.
\end{enumerate}
In a word, is $T$ is high, we move to regions of smaller values which is more random, model may produce incoherent text (off-policy noise), and if $T$ is low, model keeps generating the same high-probability responses (no diversity).

\textbullet~ \textbf{Top-p sampling (nucleus sampling):} We 
sort tokens by probability $\mpp_i$ in descending order, and take the smallest set $\cS$ so that 
\begin{align*}
  \sum_{i \in \cS}^{\mpp_i} \mpp_i \geq \mpp_{\text{threshold}},
\end{align*}
where $\mpp_{\text{threshold}}$ (e.g. $0.9$) is the top-p parameter. 
Its effect is to keep most probable tokens, while cut off the long tailed improbable tokens.

\textbullet~ \textbf{Top-k sampling:} similar to other top-K.

\textbullet~ We may use them in combination, such as temperature in $[0.7, 1]$, top $p \in [0.9, 0.95]$. 
Note that they influence not only the KL divergence but also the reward model score and the learning signal diversity.

\paragraph{Computation:} For each $t$, we compute $\pisub{\theta}{\my_t}{\mx, \my_{<t}}$ and $\pisub{\text{ref}}{\my_t}{\mx, \my_{<t}}$ and we store the corresponding 
\begin{align*}
  \text{KL}_t \rbrac{\mx, \my_{<t}, \my} = \log \pisub{\theta}{\my_t}{\mx, \my_{<t}} - \log \pisub{\text{ref}}{\my_t}{\mx, \my_{<t}},
\end{align*}
along the trajectory. 
Notice that, we have $\beta$ controlling the strength of this shaping reward (penalty reward) $-\beta \cdot \text{KL}_t \rbrac{\mx, \my_{<t}, \my} \eqdef r_t^{\text{KL}}$.
Often, large $\beta$ means strong pull toward the reference.
Finally, when we approach $t = T$, we may also include the value of $R_{\phi}(\mx, \my)$. 
The total reward per token is
\begin{align*}
  r_t = \begin{cases}
    r_t^{\text{KL}}, \qquad t < T \\
    r_t^{\text{KL}} + R_{\phi}(\mx, \my) \qquad t = T.    
  \end{cases}
\end{align*}


\subsection{The Bigger Picture in Reinforcement Learning}
In RL, theory formal setting is a Markov Decision Process (MDP):
\begin{align*}
  \rbrac{\cS, \cA, P, r, \gamma},
\end{align*}
where 
\begin{enumerate}
  \item State $\cS$: all possible situations the agent could be in.
  In RLHF, a state $\ms_t = (\mx, \my_{<t})$.
  \item Action $\cA$: choices the agent can make.
  In RLHF, it stands for picking the next token from vocabulary.
  \item Transition function $P(\ms_{t+1} \mid \ms_t, \ma_t)$: how actions change the state.
  In RLHF: appending the chosen token to the sequence.
  \item Reward function $r(\ma_t, \ms_t)$: numeric feedback,in RLHF it is to construct KL penalty in most steps and plus the terminal reward at last.
  \item Discount factor $\gamma$: how much future rewards are worth compared to immediate ones. (In RLHF we set this to $1$ since episodes\footnote{An episode is one rollout: starting from a prompt, generating tokens until we hit an end condition (EOS token or max length).} are short.) 
\end{enumerate}

\paragraph{Objective:} Assuming we have a policy $\pisub{\theta}{\ma}{\ms}$, which is a probability distribution over actions, parameterized by $\theta$ (the weights of LLM).
The objective in RL is 
\begin{align*}
  \theta^\star = \arg\max_{\theta} J(\theta),
\end{align*}
where $J(\theta)$ is defined as 
\begin{align}
  \label{eq:J}
  J(\theta) \eqdef \ExpSub{\pi_\theta}{\sum_{t=0}^{T}\gamma^tr(s_t, a_t)}.
\end{align}
That is maximize expected total reward.
Note that the expectation actually means that we are sampling episodes (tracjetories) using $\pi_{\theta}$ and for each trajectory, we compute the total discounted reward.
We average those totals over all possible trajectories, weighted by their probability of occurring under $\pi_\theta$.
For example, assume $\tau$ is a trajectory:
\begin{align*}
  \tau = \rbrac{\ms_0, \ma_0, \ms_1, \ma_1, \hdots, \ms_T, \ma_T},
\end{align*} 
since we know the probability $p_\theta(\tau)$
\begin{align*}
  p_{\theta}(\tau) = p(\ms_0)\prod_{t=0}^{T}\pi_{\theta}(\ma_t \mid \ms_t)\cdot p(\ms_{t+1} \mid \ms_t, \ma_t).
\end{align*}
we can rewrite $J(\theta)$ as 
\begin{align*}
  J(\theta) = \sum_{\tau} p_\theta(\tau)\cdot\sbrac{\sum_{t=0}^{T}\gamma^tr(s_t, a_t)}.
\end{align*}

\paragraph{Policy Gradient:}
Differentiate $J(\theta)$, denoting $R(\tau) \eqdef \sum_{t=0}^{T}\gamma^tr(s_t, a_t)$, we get 
\begin{align*}
  \nabla_{\theta} J(\theta) &= \int \nabla p_\theta(\tau) R(\tau) \quad d\tau \\
  &= \int p_\theta(\tau) \nabla_{\theta} \log p_{\theta}(\tau) \cdot R(\tau) \quad d\tau \\
  &= \ExpSub{\tau \sim p_\theta}{\nabla_\theta\log p_{\theta}(\tau)\cdot R(\tau)}.
\end{align*}
The environment dynamics $p(\ms_{t+1}\mid \ms_t, \ma_t)$ and $p(\ms_0)$ does not depend on $\theta$ which is just a const scaling on the original objective \eqref{eq:J}, we drop them and result in
\begin{align}
  \label{eq:A}
  \nabla_{\theta} J(\theta) = \ExpSub{\tau}{\sum_{t=0}^{T}\nabla_\theta \pisub{\theta}{\ma_t}{\ms_t} \cdot R(\tau)}. \tag{A}
\end{align}
This is called the \textbf{Reinforce}.
Now define the return starting at time $t$: $G_t$, which is 
\begin{align*}
  G_t \eqdef \sum_{k=t}^{T}\gamma^{k - t}r(\ms_k, \ma_k).
\end{align*}
It is a random variable depends on the future state after $t$, note that 
\begin{align*}
  R(\tau) = \underbrace{\sum_{k=0}^{t-1}\gamma^k r(\ms_k, \ma_k)}_{\text{past reward, const given $\ms_t$.}} + \gamma^t\underbrace{\sum_{k=t}^{T}\gamma^{k-t}r(\ms_k, \ma_k)}_{G_t}.
\end{align*}
For the $t$-th term in the summation in \eqref{eq:A} and \eqref{eq:J}, notice that the expectation is already conditioned on $\ma_t$ and $\ms_t$, if we remove the past reward, the gradient will not change since it is equivalent to remove a constant on $J(\theta)$, this allows us to obtain 
\begin{align}
  \label{eq:JG}
  \nabla_{\theta} J(\theta) = \ExpSub{\tau}{\sum_{t=0}^{T}\nabla_\theta \pisub{\theta}{\ma_t}{\ms_t} \cdot G_t}.
\end{align}
Now defining the action-value function $Q^{\pi}(\ms, \ma)$,
\begin{align*}
  Q^{\pi}(\ms, \ma) \eqdef \ExpSub{\pi}{G_t \mid \ms_t = \ms, \ma_t = \ma},
\end{align*}
which is the expected return if we are at $\ms$ state and take $\ma$ as action now, following policy $\pi$ thereafter.
The expectation itself is over the random future trajectory.
Using the tower property of expectation, we can further write 
\begin{align*}
  \nabla J(\theta) &= \ExpSub{\tau}{\sum_{t=0}^{T}\nabla_\theta \pisub{\theta}{\ma_t}{\ms_t} \cdot G_t}\\
  &= \ExpSub{\ms_t, \ma_t}{\sum_{t=0}^{T}\nabla_{\theta}\pisub{\theta}{\ma_t}{\ms_t}\cdot\Exp{G_t \mid \ms_t, \ma_t}} =  \ExpSub{\ms_t, \ma_t}{\sum_{t=0}^{T}\nabla_{\theta}\pisub{\theta}{\ma_t}{\ms_t}\cdot Q^{\pi_\theta}(\ms_t, \ma_t)}
\end{align*}
Now, for variance reduction, we introduce the 


\end{document}