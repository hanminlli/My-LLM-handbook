\contentsline {section}{\numberline {1}Linear Algebra}{2}{section.1}%
\contentsline {subsection}{\numberline {1.1}Column wise decomposition.}{2}{subsection.1.1}%
\contentsline {subsection}{\numberline {1.2}Row wise decomposition.}{2}{subsection.1.2}%
\contentsline {section}{\numberline {2}Multithreading}{2}{section.2}%
\contentsline {section}{\numberline {3}LLM Training}{2}{section.3}%
\contentsline {subsection}{\numberline {3.1}Scaling the logits after LLM head.}{2}{subsection.3.1}%
\contentsline {section}{\numberline {4}Attention}{3}{section.4}%
\contentsline {subsection}{\numberline {4.1}Multihead Self Attention (MHA)}{3}{subsection.4.1}%
\contentsline {subsection}{\numberline {4.2}Multi Query Attention (MQA)}{5}{subsection.4.2}%
\contentsline {subsection}{\numberline {4.3}KV cache}{5}{subsection.4.3}%
\contentsline {subsection}{\numberline {4.4}Grouped Query Attention (GQA)}{5}{subsection.4.4}%
\contentsline {subsection}{\numberline {4.5}Multihead Latent Attention (MLA)}{6}{subsection.4.5}%
\contentsline {subsection}{\numberline {4.6}Latent Transformer Block}{6}{subsection.4.6}%
\contentsline {subsection}{\numberline {4.7}Pre- and Post- Normlization}{7}{subsection.4.7}%
\contentsline {subsection}{\numberline {4.8}Dropout}{8}{subsection.4.8}%
\contentsline {paragraph}{During Training:}{8}{section*.2}%
\contentsline {paragraph}{During Inferencing:}{8}{section*.3}%
\contentsline {subsection}{\numberline {4.9}DCA}{8}{subsection.4.9}%
\contentsline {subsection}{\numberline {4.10}Linear Attention}{8}{subsection.4.10}%
\contentsline {paragraph}{Memory consumption:}{8}{section*.4}%
\contentsline {paragraph}{Linear attention:}{9}{section*.5}%
\contentsline {paragraph}{Causal (autoregressive) case:}{9}{section*.6}%
\contentsline {subsection}{\numberline {4.11}Sparse Attention}{10}{subsection.4.11}%
\contentsline {section}{\numberline {5}Normalizations}{10}{section.5}%
\contentsline {subsection}{\numberline {5.1}BatchNorm}{10}{subsection.5.1}%
\contentsline {subsection}{\numberline {5.2}RMSNorm}{10}{subsection.5.2}%
\contentsline {section}{\numberline {6}Positional Embedding}{11}{section.6}%
\contentsline {section}{\numberline {7}Precisions}{11}{section.7}%
\contentsline {paragraph}{How they (FP8) are used on H100:}{13}{section*.7}%
\contentsline {paragraph}{Scaling is not optional:}{13}{section*.8}%
\contentsline {paragraph}{Per channel vs. per tensor:}{13}{section*.9}%
\contentsline {paragraph}{Automation on H100:}{14}{section*.10}%
\contentsline {section}{\numberline {8}Encoder and Decoder Structure}{14}{section.8}%
\contentsline {paragraph}{Encoder}{14}{section*.11}%
\contentsline {paragraph}{Decoder}{16}{figure.caption.14}%
\contentsline {paragraph}{The original seq2seq transformer:}{16}{section*.15}%
\contentsline {subsection}{\numberline {8.1}Encoder only models}{17}{subsection.8.1}%
\contentsline {paragraph}{BERT pretraining in detail}{17}{section*.16}%
\contentsline {subsection}{\numberline {8.2}Decoder only models}{18}{subsection.8.2}%
\contentsline {subsection}{\numberline {8.3}Encoder-decoder models}{19}{subsection.8.3}%
\contentsline {subsection}{\numberline {8.4}Prefix-decoder models}{19}{subsection.8.4}%
\contentsline {subsection}{\numberline {8.5}Summary}{20}{subsection.8.5}%
\contentsline {subsection}{\numberline {8.6}The Linear Layers}{20}{subsection.8.6}%
\contentsline {paragraph}{Why do we put FFNs in the encoder/decoder blocks?}{21}{section*.17}%
\contentsline {paragraph}{Design choices for FFNs:}{21}{section*.18}%
\contentsline {subsection}{\numberline {8.7}Gated Linear Units}{22}{subsection.8.7}%
\contentsline {paragraph}{SwiGLU:}{22}{section*.19}%
\contentsline {paragraph}{GEGLU:}{22}{section*.20}%
\contentsline {subsection}{\numberline {8.8}Mixture-of-Experts FFNs}{22}{subsection.8.8}%
\contentsline {paragraph}{Capacity constraint:}{23}{section*.21}%
\contentsline {paragraph}{Load balancing auxiliary loss:}{24}{section*.22}%
\contentsline {paragraph}{The all to all trick:}{24}{section*.23}%
\contentsline {paragraph}{Processes and threads:}{25}{section*.24}%
\contentsline {subsection}{\numberline {8.9}Parallel Attention + FFN}{25}{subsection.8.9}%
\contentsline {section}{\numberline {9}Computing the Number of Parameters}{26}{section.9}%
\contentsline {section}{\numberline {10}Fintuning}{26}{section.10}%
\contentsline {subsection}{\numberline {10.1}Pretraining Loss (self-supervised)}{26}{subsection.10.1}%
\contentsline {subsection}{\numberline {10.2}Perplexity}{27}{subsection.10.2}%
\contentsline {subsection}{\numberline {10.3}Finetuning Loss}{27}{subsection.10.3}%
\contentsline {subsection}{\numberline {10.4}Supervised Fine-Tuning (SFT)}{27}{subsection.10.4}%
\contentsline {subsection}{\numberline {10.5}Instruction Tuning}{28}{subsection.10.5}%
\contentsline {paragraph}{Common instruction datasets}{28}{section*.31}%
\contentsline {subsection}{\numberline {10.6}Continued Pretraining}{28}{subsection.10.6}%
\contentsline {subsection}{\numberline {10.7}Parameter-Efficient Fine-Tuning (PEFT)}{28}{subsection.10.7}%
\contentsline {paragraph}{Adaptors}{29}{section*.32}%
\contentsline {paragraph}{Low-Rank Adaptation of LLMs}{29}{section*.33}%
\contentsline {subsection}{\numberline {10.8}Reinforcement Learning from Human Feedback (RLHF)}{30}{subsection.10.8}%
\contentsline {paragraph}{Stage 1: Supervised Finetuning}{31}{section*.34}%
\contentsline {paragraph}{Stage 2: Reward Modelling}{31}{section*.35}%
\contentsline {paragraph}{Stage 3: RL fine-tuning using PPO (Proximal Policy Optimization) with KL regularization.}{33}{section*.36}%
\contentsline {paragraph}{Sampling:}{34}{section*.37}%
\contentsline {paragraph}{Computation:}{34}{section*.38}%
\contentsline {subsection}{\numberline {10.9}The Bigger Picture in Reinforcement Learning (PPO)}{36}{subsection.10.9}%
\contentsline {paragraph}{Objective:}{36}{section*.39}%
\contentsline {paragraph}{Policy Gradient:}{37}{section*.40}%
\contentsline {paragraph}{From reward to policy update:}{39}{section*.43}%
\contentsline {paragraph}{Importance sampling based on old policy data.}{40}{section*.46}%
\contentsline {paragraph}{Clipping.}{41}{section*.47}%
\contentsline {paragraph}{Entropy bonus.}{41}{section*.48}%
\contentsline {paragraph}{Full PPO loss.}{41}{section*.49}%
\contentsline {paragraph}{Practical hint.}{41}{section*.50}%
\contentsline {paragraph}{Regarding the meaning of concepts}{43}{section*.51}%
\contentsline {subsection}{\numberline {10.10}PPO-KL}{43}{subsection.10.10}%
\contentsline {subsection}{\numberline {10.11}DPO (Direct Preference Optimization)}{43}{subsection.10.11}%
\contentsline {paragraph}{Why do we assume $r$ follows the specific form?}{44}{section*.54}%
\contentsline {subsection}{\numberline {10.12}GPRO (Generalized Preference Optimization)}{45}{subsection.10.12}%
\contentsline {paragraph}{The intuition:}{45}{section*.55}%
\contentsline {paragraph}{The insight of GPRO:}{47}{section*.56}%
\contentsline {paragraph}{Relation to DPO}{49}{section*.57}%
\contentsline {subsection}{\numberline {10.13}On Policy and Off Policy}{50}{subsection.10.13}%
\contentsline {paragraph}{Value based methods:}{50}{section*.58}%
\contentsline {paragraph}{SARSA: State-Action-Reward-State-Action (On policy)}{51}{section*.59}%
\contentsline {paragraph}{Q-learning (Off-policy)}{51}{section*.60}%
\contentsline {subsection}{\numberline {10.14}Domain Adaption}{51}{subsection.10.14}%
