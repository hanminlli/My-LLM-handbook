\contentsline {section}{\numberline {1}Linear Algebra}{2}{section.1}%
\contentsline {subsection}{\numberline {1.1}Column wise decomposition.}{2}{subsection.1.1}%
\contentsline {subsection}{\numberline {1.2}Row wise decomposition.}{2}{subsection.1.2}%
\contentsline {section}{\numberline {2}Multithreading}{2}{section.2}%
\contentsline {section}{\numberline {3}LLM Training}{2}{section.3}%
\contentsline {subsection}{\numberline {3.1}Scaling the logits after LLM head.}{2}{subsection.3.1}%
\contentsline {section}{\numberline {4}Attention}{3}{section.4}%
\contentsline {subsection}{\numberline {4.1}Multihead Self Attention (MHA)}{3}{subsection.4.1}%
\contentsline {subsection}{\numberline {4.2}Multi Query Attention (MQA)}{4}{subsection.4.2}%
\contentsline {subsection}{\numberline {4.3}KV cache}{5}{subsection.4.3}%
\contentsline {subsection}{\numberline {4.4}Grouped Query Attention (GQA)}{5}{subsection.4.4}%
\contentsline {subsection}{\numberline {4.5}Multihead Latent Attention (MLA)}{5}{subsection.4.5}%
\contentsline {subsection}{\numberline {4.6}Latent Transformer Block}{6}{subsection.4.6}%
\contentsline {subsection}{\numberline {4.7}Pre- and Post- Normlization}{6}{subsection.4.7}%
\contentsline {subsection}{\numberline {4.8}Dropout}{8}{subsection.4.8}%
\contentsline {paragraph}{During Training:}{8}{section*.2}%
\contentsline {paragraph}{During Inferencing:}{8}{section*.3}%
\contentsline {subsection}{\numberline {4.9}DCA}{8}{subsection.4.9}%
\contentsline {subsection}{\numberline {4.10}Linear Attention}{8}{subsection.4.10}%
\contentsline {paragraph}{Memory consumption:}{8}{section*.4}%
\contentsline {paragraph}{Linear attention:}{9}{section*.5}%
\contentsline {paragraph}{Causal (autoregressive) case:}{9}{section*.6}%
\contentsline {subsection}{\numberline {4.11}Sparse Attention}{10}{subsection.4.11}%
\contentsline {section}{\numberline {5}Normalizations}{10}{section.5}%
\contentsline {subsection}{\numberline {5.1}BatchNorm}{10}{subsection.5.1}%
\contentsline {subsection}{\numberline {5.2}RMSNorm}{10}{subsection.5.2}%
\contentsline {section}{\numberline {6}Positional Embedding}{11}{section.6}%
\contentsline {section}{\numberline {7}Precisions}{11}{section.7}%
\contentsline {paragraph}{How they (FP8) are used on H100:}{13}{section*.7}%
\contentsline {paragraph}{Scaling is not optional:}{13}{section*.8}%
\contentsline {paragraph}{Per channel vs. per tensor:}{13}{section*.9}%
\contentsline {paragraph}{Automation on H100:}{14}{section*.10}%
\contentsline {section}{\numberline {8}Encoder and Decoder Structure}{14}{section.8}%
\contentsline {paragraph}{Encoder}{14}{section*.11}%
\contentsline {paragraph}{Decoder}{16}{figure.caption.14}%
\contentsline {paragraph}{The original seq2seq transformer:}{16}{section*.15}%
\contentsline {subsection}{\numberline {8.1}Encoder only models}{17}{subsection.8.1}%
\contentsline {subsection}{\numberline {8.2}Decoder only models}{17}{subsection.8.2}%
\contentsline {subsection}{\numberline {8.3}Encoder-decoder models}{17}{subsection.8.3}%
\contentsline {subsection}{\numberline {8.4}Prefix-decoder models}{17}{subsection.8.4}%
\contentsline {subsection}{\numberline {8.5}Summary}{18}{subsection.8.5}%
\contentsline {subsection}{\numberline {8.6}The Linear Layers}{19}{subsection.8.6}%
\contentsline {paragraph}{Why do we put FFNs in the encoder/decoder blocks?}{19}{section*.16}%
\contentsline {paragraph}{Design choices for FFNs:}{19}{section*.17}%
\contentsline {subsection}{\numberline {8.7}Gated Linear Units}{20}{subsection.8.7}%
\contentsline {paragraph}{SwiGLU:}{20}{section*.18}%
\contentsline {paragraph}{GEGLU:}{20}{section*.19}%
\contentsline {subsection}{\numberline {8.8}Mixture-of-Experts FFNs}{21}{subsection.8.8}%
\contentsline {paragraph}{Capacity constraint:}{22}{section*.20}%
\contentsline {paragraph}{Load balancing auxiliary loss:}{22}{section*.21}%
\contentsline {paragraph}{The all to all trick:}{23}{section*.22}%
\contentsline {paragraph}{Processes and threads:}{23}{section*.23}%
\contentsline {subsection}{\numberline {8.9}Parallel Attention + FFN}{24}{subsection.8.9}%
\contentsline {section}{\numberline {9}Computing the Number of Parameters}{24}{section.9}%
\contentsline {section}{\numberline {10}Fintuning}{24}{section.10}%
\contentsline {subsection}{\numberline {10.1}Pretraining Loss (self-supervised)}{25}{subsection.10.1}%
\contentsline {subsection}{\numberline {10.2}Finetuning Loss}{25}{subsection.10.2}%
\contentsline {subsection}{\numberline {10.3}Supervised Fine-Tuning (SFT)}{25}{subsection.10.3}%
\contentsline {subsection}{\numberline {10.4}Instruction Tuning}{26}{subsection.10.4}%
\contentsline {paragraph}{Common instruction datasets}{26}{section*.30}%
\contentsline {subsection}{\numberline {10.5}Parameter-Efficient Fine-Tuning (PEFT)}{26}{subsection.10.5}%
\contentsline {paragraph}{Adaptors}{26}{section*.31}%
\contentsline {paragraph}{Low-Rank Adaptation of LLMs}{27}{section*.32}%
\contentsline {subsection}{\numberline {10.6}Reinforcement Learning from Human Feedback (RLHF)}{28}{subsection.10.6}%
\contentsline {paragraph}{Stage 1: Supervised Finetuning}{28}{section*.33}%
\contentsline {paragraph}{Stage 2: Reward Modelling}{29}{section*.34}%
\contentsline {paragraph}{Stage 3: RL fine-tuning using PPO (Proximal Policy Optimization) with KL regularization.}{30}{section*.35}%
\contentsline {paragraph}{Sampling:}{31}{section*.36}%
\contentsline {paragraph}{Computation:}{32}{section*.37}%
\contentsline {subsection}{\numberline {10.7}The Bigger Picture in Reinforcement Learning (PPO)}{34}{subsection.10.7}%
\contentsline {paragraph}{Objective:}{34}{section*.38}%
\contentsline {paragraph}{Policy Gradient:}{35}{section*.39}%
\contentsline {paragraph}{From reward to policy update:}{37}{section*.42}%
\contentsline {paragraph}{Importance sampling based on old policy data.}{38}{section*.45}%
\contentsline {paragraph}{Clipping.}{38}{section*.46}%
\contentsline {paragraph}{Entropy bonus.}{39}{section*.47}%
\contentsline {paragraph}{Full PPO loss.}{39}{section*.48}%
\contentsline {paragraph}{Practical hint.}{39}{section*.49}%
\contentsline {paragraph}{Regarding the meaning of concepts}{40}{section*.50}%
\contentsline {subsection}{\numberline {10.8}PPO-KL}{41}{subsection.10.8}%
\contentsline {subsection}{\numberline {10.9}DPO (Direct Preference Optimization)}{41}{subsection.10.9}%
\contentsline {paragraph}{Why do we assume $r$ follows the specific form?}{42}{section*.53}%
\contentsline {subsection}{\numberline {10.10}GPRO (Generalized Preference Optimization)}{43}{subsection.10.10}%
\contentsline {paragraph}{The intuition:}{43}{section*.54}%
\contentsline {paragraph}{The insight of GPRO:}{44}{section*.55}%
\contentsline {paragraph}{Relation to DPO}{47}{section*.56}%
\contentsline {subsection}{\numberline {10.11}On Policy and Off Policy}{47}{subsection.10.11}%
\contentsline {paragraph}{The idea of Q-learning:}{48}{section*.57}%
