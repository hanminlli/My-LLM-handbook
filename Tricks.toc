\contentsline {section}{\numberline {1}Linear Algebra}{2}{section.1}%
\contentsline {subsection}{\numberline {1.1}Column wise decomposition.}{2}{subsection.1.1}%
\contentsline {subsection}{\numberline {1.2}Row wise decomposition.}{2}{subsection.1.2}%
\contentsline {section}{\numberline {2}Multithreading}{2}{section.2}%
\contentsline {section}{\numberline {3}LLM Training}{2}{section.3}%
\contentsline {subsection}{\numberline {3.1}Scaling the logits after LLM head.}{2}{subsection.3.1}%
\contentsline {section}{\numberline {4}Attention}{3}{section.4}%
\contentsline {subsection}{\numberline {4.1}Multihead Self Attention (MHA)}{3}{subsection.4.1}%
\contentsline {subsection}{\numberline {4.2}Multi Query Attention (MQA)}{5}{subsection.4.2}%
\contentsline {subsection}{\numberline {4.3}KV cache}{5}{subsection.4.3}%
\contentsline {subsection}{\numberline {4.4}Grouped Query Attention (GQA)}{5}{subsection.4.4}%
\contentsline {subsection}{\numberline {4.5}Multihead Latent Attention (MLA)}{6}{subsection.4.5}%
\contentsline {subsection}{\numberline {4.6}Latent Transformer Block}{6}{subsection.4.6}%
\contentsline {subsection}{\numberline {4.7}Pre- and Post- Normlization}{7}{subsection.4.7}%
\contentsline {subsection}{\numberline {4.8}Dropout}{8}{subsection.4.8}%
\contentsline {paragraph}{During Training:}{8}{section*.2}%
\contentsline {paragraph}{During Inferencing:}{8}{section*.3}%
\contentsline {subsection}{\numberline {4.9}DCA}{8}{subsection.4.9}%
\contentsline {subsection}{\numberline {4.10}Linear Attention}{8}{subsection.4.10}%
\contentsline {paragraph}{Memory consumption:}{8}{section*.4}%
\contentsline {paragraph}{Linear attention:}{9}{section*.5}%
\contentsline {paragraph}{Causal (autoregressive) case:}{9}{section*.6}%
\contentsline {subsection}{\numberline {4.11}Sparse Attention}{10}{subsection.4.11}%
\contentsline {section}{\numberline {5}Normalizations}{10}{section.5}%
\contentsline {subsection}{\numberline {5.1}BatchNorm}{10}{subsection.5.1}%
\contentsline {subsection}{\numberline {5.2}RMSNorm}{10}{subsection.5.2}%
\contentsline {section}{\numberline {6}Positional Embedding}{11}{section.6}%
\contentsline {section}{\numberline {7}Precisions}{11}{section.7}%
\contentsline {paragraph}{How they (FP8) are used on H100:}{13}{section*.7}%
\contentsline {paragraph}{Scaling is not optional:}{13}{section*.8}%
\contentsline {paragraph}{Per channel vs. per tensor:}{13}{section*.9}%
\contentsline {paragraph}{Automation on H100:}{14}{section*.10}%
\contentsline {section}{\numberline {8}Encoder and Decoder Structure}{14}{section.8}%
\contentsline {paragraph}{Encoder}{14}{section*.11}%
\contentsline {paragraph}{Decoder}{16}{figure.caption.14}%
\contentsline {paragraph}{The original seq2seq transformer:}{16}{section*.15}%
\contentsline {subsection}{\numberline {8.1}Encoder only models}{17}{subsection.8.1}%
\contentsline {paragraph}{BERT pretraining in detail}{17}{section*.16}%
\contentsline {subsection}{\numberline {8.2}Decoder only models}{18}{subsection.8.2}%
\contentsline {subsection}{\numberline {8.3}Encoder-decoder models}{19}{subsection.8.3}%
\contentsline {subsection}{\numberline {8.4}Prefix-decoder models}{19}{subsection.8.4}%
\contentsline {subsection}{\numberline {8.5}Summary}{20}{subsection.8.5}%
\contentsline {subsection}{\numberline {8.6}The Linear Layers}{20}{subsection.8.6}%
\contentsline {paragraph}{Why do we put FFNs in the encoder/decoder blocks?}{21}{section*.17}%
\contentsline {paragraph}{Design choices for FFNs:}{21}{section*.18}%
\contentsline {subsection}{\numberline {8.7}Gated Linear Units}{22}{subsection.8.7}%
\contentsline {paragraph}{SwiGLU:}{22}{section*.19}%
\contentsline {paragraph}{GEGLU:}{22}{section*.20}%
\contentsline {subsection}{\numberline {8.8}Mixture-of-Experts FFNs}{22}{subsection.8.8}%
\contentsline {paragraph}{Capacity constraint:}{23}{section*.21}%
\contentsline {paragraph}{Load balancing auxiliary loss:}{24}{section*.22}%
\contentsline {paragraph}{The all to all trick:}{24}{section*.23}%
\contentsline {paragraph}{Processes and threads:}{25}{section*.24}%
\contentsline {subsection}{\numberline {8.9}Parallel Attention + FFN}{25}{subsection.8.9}%
\contentsline {section}{\numberline {9}Computing the Number of Parameters}{26}{section.9}%
\contentsline {section}{\numberline {10}Fintuning}{26}{section.10}%
\contentsline {subsection}{\numberline {10.1}Pretraining Loss (self-supervised)}{26}{subsection.10.1}%
\contentsline {subsection}{\numberline {10.2}Perplexity}{27}{subsection.10.2}%
\contentsline {subsection}{\numberline {10.3}Finetuning Loss}{27}{subsection.10.3}%
\contentsline {subsubsection}{\numberline {10.3.1}Gradient Checkpointing}{27}{subsubsection.10.3.1}%
\contentsline {subsubsection}{\numberline {10.3.2}Gradient Accumulation}{28}{subsubsection.10.3.2}%
\contentsline {subsection}{\numberline {10.4}Supervised Fine-Tuning (SFT)}{28}{subsection.10.4}%
\contentsline {subsubsection}{\numberline {10.4.1}Parallelism}{28}{subsubsection.10.4.1}%
\contentsline {paragraph}{PyTorch DDP:}{28}{section*.31}%
\contentsline {paragraph}{Pros:}{28}{section*.32}%
\contentsline {paragraph}{Cons:}{28}{section*.33}%
\contentsline {paragraph}{Hugging face accelerate:}{28}{section*.34}%
\contentsline {paragraph}{DeepSpeed ZeRO}{29}{section*.35}%
\contentsline {paragraph}{PyTorch FSDP}{29}{section*.36}%
\contentsline {paragraph}{HuggingFace Trainer}{29}{section*.37}%
\contentsline {subsection}{\numberline {10.5}Instruction Tuning}{29}{subsection.10.5}%
\contentsline {paragraph}{Common instruction datasets}{29}{section*.38}%
\contentsline {subsection}{\numberline {10.6}Continued Pretraining}{30}{subsection.10.6}%
\contentsline {subsection}{\numberline {10.7}Parameter-Efficient Fine-Tuning (PEFT)}{30}{subsection.10.7}%
\contentsline {paragraph}{Adaptors}{30}{section*.39}%
\contentsline {paragraph}{Low-Rank Adaptation of LLMs}{30}{section*.40}%
\contentsline {subsection}{\numberline {10.8}Reinforcement Learning from Human Feedback (RLHF)}{32}{subsection.10.8}%
\contentsline {paragraph}{Stage 1: Supervised Finetuning}{32}{section*.41}%
\contentsline {paragraph}{Stage 2: Reward Modelling}{32}{section*.42}%
\contentsline {paragraph}{Stage 3: RL fine-tuning using PPO (Proximal Policy Optimization) with KL regularization.}{34}{section*.43}%
\contentsline {paragraph}{Sampling:}{35}{section*.44}%
\contentsline {paragraph}{Computation:}{36}{section*.45}%
\contentsline {subsection}{\numberline {10.9}The Bigger Picture in Reinforcement Learning (PPO)}{37}{subsection.10.9}%
\contentsline {paragraph}{Objective:}{38}{section*.46}%
\contentsline {paragraph}{Policy Gradient:}{38}{section*.47}%
\contentsline {paragraph}{From reward to policy update:}{40}{section*.50}%
\contentsline {paragraph}{Importance sampling based on old policy data.}{42}{section*.53}%
\contentsline {paragraph}{Clipping.}{42}{section*.54}%
\contentsline {paragraph}{Entropy bonus.}{42}{section*.55}%
\contentsline {paragraph}{Full PPO loss.}{42}{section*.56}%
\contentsline {paragraph}{Practical hint.}{43}{section*.57}%
\contentsline {paragraph}{Regarding the meaning of concepts}{44}{section*.58}%
\contentsline {subsection}{\numberline {10.10}PPO-KL}{44}{subsection.10.10}%
\contentsline {subsection}{\numberline {10.11}DPO (Direct Preference Optimization)}{45}{subsection.10.11}%
\contentsline {paragraph}{Why do we assume $r$ follows the specific form?}{46}{section*.61}%
\contentsline {subsection}{\numberline {10.12}GPRO (Generalized Preference Optimization)}{46}{subsection.10.12}%
\contentsline {paragraph}{The intuition:}{47}{section*.62}%
\contentsline {paragraph}{The insight of GPRO:}{48}{section*.63}%
\contentsline {paragraph}{Relation to DPO}{50}{section*.64}%
\contentsline {subsection}{\numberline {10.13}On Policy and Off Policy}{51}{subsection.10.13}%
\contentsline {paragraph}{Value based methods:}{52}{section*.65}%
\contentsline {paragraph}{SARSA: State-Action-Reward-State-Action (On policy)}{52}{section*.66}%
\contentsline {paragraph}{Q-learning (Off-policy)}{52}{section*.67}%
\contentsline {subsection}{\numberline {10.14}Domain Adaption}{53}{subsection.10.14}%
