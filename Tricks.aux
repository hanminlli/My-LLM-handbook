\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Linear Algebra}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Column wise decomposition.}{2}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Row wise decomposition.}{2}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Multithreading}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}LLM Training}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Scaling the logits after LLM head.}{2}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Attention}{3}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Multihead Self Attention (MHA)}{3}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Multi Query Attention (MQA)}{5}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}KV cache}{5}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Grouped Query Attention (GQA)}{5}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Multihead Latent Attention (MLA)}{6}{subsection.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Latent Transformer Block}{6}{subsection.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}Pre- and Post- Normlization}{7}{subsection.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8}Dropout}{8}{subsection.4.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{During Training:}{8}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{During Inferencing:}{8}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9}DCA}{8}{subsection.4.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.10}Linear Attention}{8}{subsection.4.10}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{table1}{{\caption@xref {table1}{ on input line 355}}{8}{Linear Attention}{subsection.4.10}{}}
\newlabel{table1@cref}{{[subsection][10][4]4.10}{[1][8][]8}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Computation complexity of the main steps in a standard self-attention block. Here $B$ = batch size, $L$ = sequence length, $H$ = number of heads, $d_{\text  {head}}$ = head dimension, and $d_{\text  {model}} = H \cdot d_{\text  {head}}$.}}{8}{table.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Memory consumption:}{8}{section*.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Asymptotic compute and memory complexity for different attention scenarios. Here $B$ = batch size, $L$ = sequence length (train), $H$ = number of heads, $d_{\text  {head}}$ = per-head dimension, $d_{\text  {model}} = H \cdot d_{\text  {head}}$, $L_q$/$L_k$ = query/key lengths for cross-attention, and $t$ = current decoding step in autoregressive inference. Autoregressive decoding is with KV cache.}}{9}{table.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Linear attention:}{9}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Causal (autoregressive) case:}{9}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11}Sparse Attention}{10}{subsection.4.11}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Normalizations}{10}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}BatchNorm}{10}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}RMSNorm}{10}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Positional Embedding}{11}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Precisions}{11}{section.7}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Mid-/high-precision dtypes widely used during LLM training.$^\dagger $Smallest positive \emph  {normal} value to largest finite.$^{*}$TF32 is a compute mode; tensors stored as FP32.}}{12}{table.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Very low-precision formats used for efficient training research (FP8) or deployment quantisation (INT).}}{12}{table.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How they (FP8) are used on H100:}{13}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Scaling is not optional:}{13}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Per channel vs. per tensor:}{13}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Automation on H100:}{14}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Encoder and Decoder Structure}{14}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Encoder}{14}{section*.11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Pre-LN Transformer \textbf  {Decoder} block. Each sublayer consumes a LayerNormed input. The masked self-attention applies a \emph  {causal mask} (no future tokens). Cross-attention queries come from the decoder stream, while keys/values are derived from the encoder memory $\mathbf  {M}$ (encoder hidden states: $\mathbf  {K}=\mathbf  {W}_\mathbf  {K}\mathbf  {M}$, $\mathbf  {V}=\mathbf  {W}_\mathbf  {V}\mathbf  {M}$).}}{15}{figure.caption.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Decoder}{16}{figure.caption.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The original seq2seq transformer:}{16}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Encoder only models}{17}{subsection.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{BERT pretraining in detail}{17}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Decoder only models}{18}{subsection.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Encoder-decoder models}{19}{subsection.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Prefix-decoder models}{19}{subsection.8.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5}Summary}{20}{subsection.8.5}\protected@file@percent }
\newlabel{tablex1}{{\caption@xref {tablex1}{ on input line 1003}}{20}{Summary}{subsection.8.5}{}}
\newlabel{tablex1@cref}{{[subsection][5][8]8.5}{[1][20][]20}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Comparison of encoder-only, decoder-only, and encoder--decoder Transformer architectures.}}{20}{table.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.6}The Linear Layers}{20}{subsection.8.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why do we put FFNs in the encoder/decoder blocks?}{21}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Design choices for FFNs:}{21}{section*.18}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.7}Gated Linear Units}{22}{subsection.8.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{SwiGLU:}{22}{section*.19}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{GEGLU:}{22}{section*.20}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.8}Mixture-of-Experts FFNs}{22}{subsection.8.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Capacity constraint:}{23}{section*.21}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Load balancing auxiliary loss:}{24}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The all to all trick:}{24}{section*.23}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Processes and threads:}{25}{section*.24}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.9}Parallel Attention + FFN}{25}{subsection.8.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Computing the Number of Parameters}{26}{section.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10}Fintuning}{26}{section.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}Pretraining Loss (self-supervised)}{26}{subsection.10.1}\protected@file@percent }
\newlabel{eq:CLM}{{{CLM}}{26}{Pretraining Loss (self-supervised)}{AMS.26}{}}
\newlabel{eq:CLM@cref}{{[equation][2147483647][]{CLM}}{[1][26][]26}}
\newlabel{eq:CrossEntropy}{{{Cross Entropy}}{26}{Pretraining Loss (self-supervised)}{AMS.28}{}}
\newlabel{eq:CrossEntropy@cref}{{[equation][2147483647][]{Cross Entropy}}{[1][26][]26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2}Perplexity}{27}{subsection.10.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3}Finetuning Loss}{27}{subsection.10.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.3.1}Gradient Checkpointing}{27}{subsubsection.10.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.3.2}Gradient Accumulation}{28}{subsubsection.10.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4}Supervised Fine-Tuning (SFT)}{28}{subsection.10.4}\protected@file@percent }
\newlabel{eq:lossSFT}{{{SFT}}{28}{Supervised Fine-Tuning (SFT)}{AMS.30}{}}
\newlabel{eq:lossSFT@cref}{{[equation][2147483647][]{SFT}}{[1][28][]28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5}Instruction Tuning}{28}{subsection.10.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Common instruction datasets}{28}{section*.31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.6}Continued Pretraining}{29}{subsection.10.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.7}Parameter-Efficient Fine-Tuning (PEFT)}{29}{subsection.10.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Adaptors}{29}{section*.32}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Low-Rank Adaptation of LLMs}{29}{section*.33}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.8}Reinforcement Learning from Human Feedback (RLHF)}{31}{subsection.10.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Stage 1: Supervised Finetuning}{31}{section*.34}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Stage 2: Reward Modelling}{31}{section*.35}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Stage 3: RL fine-tuning using PPO (Proximal Policy Optimization) with KL regularization.}{33}{section*.36}\protected@file@percent }
\newlabel{eq-obj-RL}{{12}{33}{Stage 3: RL fine-tuning using PPO (Proximal Policy Optimization) with KL regularization}{equation.10.12}{}}
\newlabel{eq-obj-RL@cref}{{[equation][12][]12}{[1][33][]33}}
\@writefile{toc}{\contentsline {paragraph}{Sampling:}{34}{section*.37}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Computation:}{35}{section*.38}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.9}The Bigger Picture in Reinforcement Learning (PPO)}{36}{subsection.10.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Objective:}{37}{section*.39}\protected@file@percent }
\newlabel{eq:J}{{13}{37}{Objective:}{equation.10.13}{}}
\newlabel{eq:J@cref}{{[equation][13][]13}{[1][36][]37}}
\@writefile{toc}{\contentsline {paragraph}{Policy Gradient:}{37}{section*.40}\protected@file@percent }
\newlabel{eq:A}{{{A}}{37}{Policy Gradient:}{AMS.42}{}}
\newlabel{eq:A@cref}{{[equation][2147483647][]{A}}{[1][37][]37}}
\newlabel{eq:JG}{{14}{38}{Policy Gradient:}{equation.10.14}{}}
\newlabel{eq:JG@cref}{{[equation][14][]14}{[1][38][]38}}
\@writefile{toc}{\contentsline {paragraph}{From reward to policy update:}{39}{section*.43}\protected@file@percent }
\newlabel{eq:GHAT}{{16}{40}{From reward to policy update:}{equation.10.16}{}}
\newlabel{eq:GHAT@cref}{{[equation][16][]16}{[1][40][]40}}
\newlabel{eq:policy-grad}{{{Policy Gradient}}{40}{From reward to policy update:}{AMS.45}{}}
\newlabel{eq:policy-grad@cref}{{[equation][2147483647][]{Policy Gradient}}{[1][40][]40}}
\@writefile{toc}{\contentsline {paragraph}{Importance sampling based on old policy data.}{41}{section*.46}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Clipping.}{41}{section*.47}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Entropy bonus.}{41}{section*.48}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Full PPO loss.}{41}{section*.49}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Practical hint.}{42}{section*.50}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Regarding the meaning of concepts}{43}{section*.51}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.10}PPO-KL}{43}{subsection.10.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.11}DPO (Direct Preference Optimization)}{44}{subsection.10.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why do we assume $r$ follows the specific form?}{45}{section*.54}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.12}GPRO (Generalized Preference Optimization)}{45}{subsection.10.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The intuition:}{46}{section*.55}\protected@file@percent }
\newlabel{eq:change-measure}{{17}{46}{The intuition:}{equation.10.17}{}}
\newlabel{eq:change-measure@cref}{{[equation][17][]17}{[1][46][]46}}
\@writefile{toc}{\contentsline {paragraph}{The insight of GPRO:}{47}{section*.56}\protected@file@percent }
\newlabel{eq:org-1}{{18}{47}{The insight of GPRO:}{equation.10.18}{}}
\newlabel{eq:org-1@cref}{{[equation][18][]18}{[1][47][]47}}
\@writefile{toc}{\contentsline {paragraph}{Relation to DPO}{49}{section*.57}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.13}On Policy and Off Policy}{50}{subsection.10.13}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Value based methods:}{51}{section*.58}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{SARSA: State-Action-Reward-State-Action (On policy)}{51}{section*.59}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Q-learning (Off-policy)}{51}{section*.60}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.14}Domain Adaption}{52}{subsection.10.14}\protected@file@percent }
\newlabel{LastPage}{{}{52}{}{page.52}{}}
\xdef\lastpage@lastpage{52}
\xdef\lastpage@lastpageHy{52}
\gdef \@abspage@last{52}
