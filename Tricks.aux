\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Linear Algebra}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Column wise decomposition.}{1}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Row wise decomposition.}{1}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Multithreading}{1}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}LLM Training}{1}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Scaling the logits after LLM head.}{1}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Attention}{2}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Multihead Self Attention (MHA)}{2}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Multi Query Attention (MQA)}{3}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}KV cache}{4}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Grouped Query Attention (GQA)}{4}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Multihead Latent Attention (MLA)}{4}{subsection.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Latent Transformer Block}{5}{subsection.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}Pre- and Post- Normlization}{5}{subsection.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8}Dropout}{7}{subsection.4.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{During Training:}{7}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{During Inferencing:}{7}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9}DCA}{7}{subsection.4.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.10}Linear Attention}{7}{subsection.4.10}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{table1}{{\caption@xref {table1}{ on input line 353}}{7}{Linear Attention}{subsection.4.10}{}}
\newlabel{table1@cref}{{[subsection][10][4]4.10}{[1][7][]7}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Computation complexity of the main steps in a standard self-attention block. Here $B$ = batch size, $L$ = sequence length, $H$ = number of heads, $d_{\text  {head}}$ = head dimension, and $d_{\text  {model}} = H \cdot d_{\text  {head}}$.}}{7}{table.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Memory consumption:}{7}{section*.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Asymptotic compute and memory complexity for different attention scenarios. Here $B$ = batch size, $L$ = sequence length (train), $H$ = number of heads, $d_{\text  {head}}$ = per-head dimension, $d_{\text  {model}} = H \cdot d_{\text  {head}}$, $L_q$/$L_k$ = query/key lengths for cross-attention, and $t$ = current decoding step in autoregressive inference. Autoregressive decoding is with KV cache.}}{8}{table.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Linear attention:}{8}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11}Sparse Attention}{8}{subsection.4.11}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Normalizations}{8}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}BatchNorm}{8}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}RMSNorm}{9}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Positional Embedding}{10}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Precisions}{10}{section.7}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Mid-/high-precision dtypes widely used during LLM training.$^\dagger $Smallest positive \emph  {normal} value to largest finite.$^{*}$TF32 is a compute mode; tensors stored as FP32.}}{10}{table.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Very low-precision formats used for efficient training research (FP8) or deployment quantisation (INT).}}{10}{table.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How they (FP8) are used on H100:}{11}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Scaling is not optional:}{12}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Per channel vs. per tensor:}{12}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Automation on H100:}{12}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Encoder and Decoder Structure}{12}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Encoder}{12}{section*.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Pre-LN Transformer \textbf  {Decoder} block. Each sublayer consumes a LayerNormed input. The masked self-attention applies a \emph  {causal mask} (no future tokens). Cross-attention queries come from the decoder stream, while keys/values are derived from the encoder memory $\mathbf  {M}$ (encoder hidden states: $\mathbf  {K}=\mathbf  {W}_\mathbf  {K}\mathbf  {M}$, $\mathbf  {V}=\mathbf  {W}_\mathbf  {V}\mathbf  {M}$).}}{13}{figure.caption.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Decoder}{14}{figure.caption.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The original seq2seq transformer:}{14}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Encoder only models}{15}{subsection.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Decoder only models}{15}{subsection.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Encoder-decoder models}{15}{subsection.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Prefix-decoder models}{15}{subsection.8.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5}Summary}{16}{subsection.8.5}\protected@file@percent }
\newlabel{tablex1}{{\caption@xref {tablex1}{ on input line 896}}{17}{Summary}{subsection.8.5}{}}
\newlabel{tablex1@cref}{{[subsection][5][8]8.5}{[1][16][]17}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Comparison of encoder-only, decoder-only, and encoder--decoder Transformer architectures.}}{17}{table.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.6}The Linear Layers}{17}{subsection.8.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why do we put FFNs in the encoder/decoder blocks?}{17}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Design choices for FFNs:}{17}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.7}Gated Linear Units}{18}{subsection.8.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{SwiGLU:}{18}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{GEGLU:}{18}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.8}Mixture-of-Experts FFNs}{19}{subsection.8.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Capacity constraint:}{20}{section*.18}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Load balancing auxiliary loss:}{20}{section*.19}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The all to all trick:}{21}{section*.20}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.9}Parallel Attention + FFN}{21}{subsection.8.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Computing the Number of Parameters}{22}{section.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10}Fintuning}{22}{section.10}\protected@file@percent }
\newlabel{LastPage}{{}{22}{}{page.22}{}}
\xdef\lastpage@lastpage{22}
\xdef\lastpage@lastpageHy{22}
\gdef \@abspage@last{22}
