\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Linear Algebra}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Column wise decomposition.}{1}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Row wise decomposition.}{1}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Multithreading}{1}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}LLM Training}{1}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Scaling the logits after LLM head.}{1}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Attention}{2}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Multihead Self Attention (MHA)}{2}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Multi Query Attention (MQA)}{3}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}KV cache}{4}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Grouped Query Attention (GQA)}{4}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Multihead Latent Attention (MLA)}{4}{subsection.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Latent Transformer Block}{5}{subsection.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}Pre- and Post- Normlization}{5}{subsection.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8}Dropout}{7}{subsection.4.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{During Training:}{7}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{During Inferencing:}{7}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9}DCA}{7}{subsection.4.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.10}Linear Attention}{7}{subsection.4.10}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{table1}{{\caption@xref {table1}{ on input line 353}}{7}{Linear Attention}{subsection.4.10}{}}
\newlabel{table1@cref}{{[subsection][10][4]4.10}{[1][7][]7}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Computation complexity of the main steps in a standard self-attention block. Here $B$ = batch size, $L$ = sequence length, $H$ = number of heads, $d_{\text  {head}}$ = head dimension, and $d_{\text  {model}} = H \cdot d_{\text  {head}}$.}}{7}{table.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Memory consumption:}{7}{section*.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Asymptotic compute and memory complexity for different attention scenarios. Here $B$ = batch size, $L$ = sequence length (train), $H$ = number of heads, $d_{\text  {head}}$ = per-head dimension, $d_{\text  {model}} = H \cdot d_{\text  {head}}$, $L_q$/$L_k$ = query/key lengths for cross-attention, and $t$ = current decoding step in autoregressive inference. Autoregressive decoding is with KV cache.}}{8}{table.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Linear attention:}{8}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Causal (autoregressive) case:}{8}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11}Sparse Attention}{9}{subsection.4.11}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Normalizations}{9}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}BatchNorm}{9}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}RMSNorm}{9}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Positional Embedding}{10}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Precisions}{10}{section.7}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Mid-/high-precision dtypes widely used during LLM training.$^\dagger $Smallest positive \emph  {normal} value to largest finite.$^{*}$TF32 is a compute mode; tensors stored as FP32.}}{11}{table.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Very low-precision formats used for efficient training research (FP8) or deployment quantisation (INT).}}{11}{table.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How they (FP8) are used on H100:}{12}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Scaling is not optional:}{12}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Per channel vs. per tensor:}{12}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Automation on H100:}{13}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Encoder and Decoder Structure}{13}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Encoder}{13}{section*.10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Pre-LN Transformer \textbf  {Decoder} block. Each sublayer consumes a LayerNormed input. The masked self-attention applies a \emph  {causal mask} (no future tokens). Cross-attention queries come from the decoder stream, while keys/values are derived from the encoder memory $\mathbf  {M}$ (encoder hidden states: $\mathbf  {K}=\mathbf  {W}_\mathbf  {K}\mathbf  {M}$, $\mathbf  {V}=\mathbf  {W}_\mathbf  {V}\mathbf  {M}$).}}{14}{figure.caption.13}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Decoder}{15}{figure.caption.13}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The original seq2seq transformer:}{15}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Encoder only models}{16}{subsection.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Decoder only models}{16}{subsection.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Encoder-decoder models}{16}{subsection.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Prefix-decoder models}{16}{subsection.8.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5}Summary}{17}{subsection.8.5}\protected@file@percent }
\newlabel{tablex1}{{\caption@xref {tablex1}{ on input line 920}}{18}{Summary}{subsection.8.5}{}}
\newlabel{tablex1@cref}{{[subsection][5][8]8.5}{[1][17][]18}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Comparison of encoder-only, decoder-only, and encoder--decoder Transformer architectures.}}{18}{table.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.6}The Linear Layers}{18}{subsection.8.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why do we put FFNs in the encoder/decoder blocks?}{18}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Design choices for FFNs:}{18}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.7}Gated Linear Units}{19}{subsection.8.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{SwiGLU:}{19}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{GEGLU:}{19}{section*.18}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.8}Mixture-of-Experts FFNs}{20}{subsection.8.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Capacity constraint:}{21}{section*.19}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Load balancing auxiliary loss:}{21}{section*.20}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The all to all trick:}{22}{section*.21}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Processes and threads:}{22}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.9}Parallel Attention + FFN}{23}{subsection.8.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Computing the Number of Parameters}{24}{section.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10}Fintuning}{24}{section.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}Pretraining Loss (self-supervised)}{24}{subsection.10.1}\protected@file@percent }
\newlabel{eq:CLM}{{{CLM}}{24}{Pretraining Loss (self-supervised)}{AMS.24}{}}
\newlabel{eq:CLM@cref}{{[equation][2147483647][]{CLM}}{[1][24][]24}}
\newlabel{eq:CrossEntropy}{{{Cross Entropy}}{24}{Pretraining Loss (self-supervised)}{AMS.26}{}}
\newlabel{eq:CrossEntropy@cref}{{[equation][2147483647][]{Cross Entropy}}{[1][24][]24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2}Finetuning Loss}{24}{subsection.10.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3}Supervised Fine-Tuning (SFT)}{24}{subsection.10.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4}Parameter-Efficient Fine-Tuning (PEFT)}{25}{subsection.10.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Adaptors}{25}{section*.29}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Low-Rank Adaptation of LLMs}{25}{section*.30}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5}Reinforcement Learning from Human Feedback (RLHF)}{27}{subsection.10.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Stage 1: Supervised Finetuning}{27}{section*.31}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Stage 2: Reward Modelling}{27}{section*.32}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Stage 3: RL fine-tuning with PPO}{29}{section*.33}\protected@file@percent }
\newlabel{LastPage}{{}{29}{}{page.29}{}}
\xdef\lastpage@lastpage{29}
\xdef\lastpage@lastpageHy{29}
\gdef \@abspage@last{29}
