\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Linear Algebra}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Column wise decomposition.}{1}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Row wise decomposition.}{1}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}LLM Training}{1}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Scaling the logits after LLM head.}{1}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Attention}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Multihead Self Attention (MHA)}{2}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Multi Query Attention (MQA)}{3}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}KV cache}{3}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Grouped Query Attention (GQA)}{4}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Multihead Latent Attention (MLA)}{4}{subsection.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Latent Transformer Block}{4}{subsection.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}Pre- and Post- Normlization}{5}{subsection.3.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8}Dropout}{6}{subsection.3.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{During Training:}{6}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{During Inferencing:}{6}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9}DCA}{6}{subsection.3.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.10}Linear Attention}{6}{subsection.3.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.11}Sparse Attention}{6}{subsection.3.11}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Normalizations}{6}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}BatchNorm}{7}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}RMSNorm}{7}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Positional Embedding}{8}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Precisions}{8}{section.6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Mid-/high-precision dtypes widely used during LLM training.$^\dagger $Smallest positive \emph  {normal} value to largest finite.$^{*}$TF32 is a compute mode; tensors stored as FP32.}}{8}{table.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Very low-precision formats used for efficient training research (FP8) or deployment quantisation (INT).}}{8}{table.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How they (FP8) are used on H100:}{10}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Scaling is not optional:}{10}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Per channel vs. per tensor:}{10}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Automation on H100:}{10}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Encoder and Decoder Structure}{11}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Encoder}{11}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Decoder}{11}{figure.caption.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The original seq2seq transformer:}{12}{section*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Encoder only models}{12}{subsection.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Decoder only models}{13}{subsection.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Encoder-decoder models}{13}{subsection.7.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Comparison of encoder-only, decoder-only, and encoder--decoder Transformer architectures.}}{13}{table.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}The Linear Layers}{14}{subsection.7.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why do we put FFNs in the encoder/decoder blocks?}{14}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Design choices for FFNs:}{14}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Computing the Number of Parameters}{15}{section.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Pre-LN Transformer \textbf  {Decoder} block. Each sublayer consumes a LayerNormed input. The masked self-attention applies a \emph  {causal mask} (no future tokens). Cross-attention queries come from the decoder stream, while keys/values are derived from the encoder memory $\mathbf  {M}$ (encoder hidden states: $\mathbf  {K}=\mathbf  {W}_\mathbf  {K}\mathbf  {M}$, $\mathbf  {V}=\mathbf  {W}_\mathbf  {V}\mathbf  {M}$).}}{16}{figure.caption.10}\protected@file@percent }
\newlabel{LastPage}{{8}{16}{Computing the Number of Parameters}{page.16}{}}
\gdef\lastpage@lastpage{16}
\gdef\lastpage@lastpageHy{16}
\gdef \@abspage@last{16}
