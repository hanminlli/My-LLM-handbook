\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Linear Algebra}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Column wise decomposition.}{2}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Row wise decomposition.}{2}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Multithreading}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}LLM Training}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Scaling the logits after LLM head.}{2}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Attention}{3}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Multihead Self Attention (MHA)}{3}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Multi Query Attention (MQA)}{4}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}KV cache}{5}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Grouped Query Attention (GQA)}{5}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Multihead Latent Attention (MLA)}{5}{subsection.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Latent Transformer Block}{6}{subsection.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}Pre- and Post- Normlization}{6}{subsection.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8}Dropout}{8}{subsection.4.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{During Training:}{8}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{During Inferencing:}{8}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9}DCA}{8}{subsection.4.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.10}Linear Attention}{8}{subsection.4.10}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{table1}{{\caption@xref {table1}{ on input line 355}}{8}{Linear Attention}{subsection.4.10}{}}
\newlabel{table1@cref}{{[subsection][10][4]4.10}{[1][8][]8}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Computation complexity of the main steps in a standard self-attention block. Here $B$ = batch size, $L$ = sequence length, $H$ = number of heads, $d_{\text  {head}}$ = head dimension, and $d_{\text  {model}} = H \cdot d_{\text  {head}}$.}}{8}{table.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Memory consumption:}{8}{section*.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Asymptotic compute and memory complexity for different attention scenarios. Here $B$ = batch size, $L$ = sequence length (train), $H$ = number of heads, $d_{\text  {head}}$ = per-head dimension, $d_{\text  {model}} = H \cdot d_{\text  {head}}$, $L_q$/$L_k$ = query/key lengths for cross-attention, and $t$ = current decoding step in autoregressive inference. Autoregressive decoding is with KV cache.}}{9}{table.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Linear attention:}{9}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Causal (autoregressive) case:}{9}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11}Sparse Attention}{10}{subsection.4.11}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Normalizations}{10}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}BatchNorm}{10}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}RMSNorm}{10}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Positional Embedding}{11}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Precisions}{11}{section.7}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Mid-/high-precision dtypes widely used during LLM training.$^\dagger $Smallest positive \emph  {normal} value to largest finite.$^{*}$TF32 is a compute mode; tensors stored as FP32.}}{12}{table.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Very low-precision formats used for efficient training research (FP8) or deployment quantisation (INT).}}{12}{table.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How they (FP8) are used on H100:}{13}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Scaling is not optional:}{13}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Per channel vs. per tensor:}{13}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Automation on H100:}{14}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Encoder and Decoder Structure}{14}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Encoder}{14}{section*.11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Pre-LN Transformer \textbf  {Decoder} block. Each sublayer consumes a LayerNormed input. The masked self-attention applies a \emph  {causal mask} (no future tokens). Cross-attention queries come from the decoder stream, while keys/values are derived from the encoder memory $\mathbf  {M}$ (encoder hidden states: $\mathbf  {K}=\mathbf  {W}_\mathbf  {K}\mathbf  {M}$, $\mathbf  {V}=\mathbf  {W}_\mathbf  {V}\mathbf  {M}$).}}{15}{figure.caption.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Decoder}{16}{figure.caption.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The original seq2seq transformer:}{16}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Encoder only models}{17}{subsection.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Decoder only models}{17}{subsection.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Encoder-decoder models}{17}{subsection.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Prefix-decoder models}{17}{subsection.8.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5}Summary}{18}{subsection.8.5}\protected@file@percent }
\newlabel{tablex1}{{\caption@xref {tablex1}{ on input line 922}}{19}{Summary}{subsection.8.5}{}}
\newlabel{tablex1@cref}{{[subsection][5][8]8.5}{[1][18][]19}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Comparison of encoder-only, decoder-only, and encoder--decoder Transformer architectures.}}{19}{table.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.6}The Linear Layers}{19}{subsection.8.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why do we put FFNs in the encoder/decoder blocks?}{19}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Design choices for FFNs:}{19}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.7}Gated Linear Units}{20}{subsection.8.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{SwiGLU:}{20}{section*.18}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{GEGLU:}{20}{section*.19}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.8}Mixture-of-Experts FFNs}{21}{subsection.8.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Capacity constraint:}{22}{section*.20}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Load balancing auxiliary loss:}{22}{section*.21}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The all to all trick:}{23}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Processes and threads:}{23}{section*.23}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.9}Parallel Attention + FFN}{24}{subsection.8.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Computing the Number of Parameters}{24}{section.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10}Fintuning}{24}{section.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}Pretraining Loss (self-supervised)}{25}{subsection.10.1}\protected@file@percent }
\newlabel{eq:CLM}{{{CLM}}{25}{Pretraining Loss (self-supervised)}{AMS.25}{}}
\newlabel{eq:CLM@cref}{{[equation][2147483647][]{CLM}}{[1][25][]25}}
\newlabel{eq:CrossEntropy}{{{Cross Entropy}}{25}{Pretraining Loss (self-supervised)}{AMS.27}{}}
\newlabel{eq:CrossEntropy@cref}{{[equation][2147483647][]{Cross Entropy}}{[1][25][]25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2}Finetuning Loss}{25}{subsection.10.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3}Supervised Fine-Tuning (SFT)}{25}{subsection.10.3}\protected@file@percent }
\newlabel{eq:lossSFT}{{{SFT}}{25}{Supervised Fine-Tuning (SFT)}{AMS.29}{}}
\newlabel{eq:lossSFT@cref}{{[equation][2147483647][]{SFT}}{[1][25][]25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4}Instruction Tuning}{26}{subsection.10.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Common instruction datasets}{26}{section*.30}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5}Parameter-Efficient Fine-Tuning (PEFT)}{26}{subsection.10.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Adaptors}{26}{section*.31}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Low-Rank Adaptation of LLMs}{27}{section*.32}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.6}Reinforcement Learning from Human Feedback (RLHF)}{28}{subsection.10.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Stage 1: Supervised Finetuning}{28}{section*.33}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Stage 2: Reward Modelling}{29}{section*.34}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Stage 3: RL fine-tuning using PPO (Proximal Policy Optimization) with KL regularization.}{30}{section*.35}\protected@file@percent }
\newlabel{eq-obj-RL}{{12}{30}{Stage 3: RL fine-tuning using PPO (Proximal Policy Optimization) with KL regularization}{equation.10.12}{}}
\newlabel{eq-obj-RL@cref}{{[equation][12][]12}{[1][30][]30}}
\@writefile{toc}{\contentsline {paragraph}{Sampling:}{31}{section*.36}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Computation:}{32}{section*.37}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.7}The Bigger Picture in Reinforcement Learning (PPO)}{34}{subsection.10.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Objective:}{34}{section*.38}\protected@file@percent }
\newlabel{eq:J}{{13}{34}{Objective:}{equation.10.13}{}}
\newlabel{eq:J@cref}{{[equation][13][]13}{[1][34][]34}}
\@writefile{toc}{\contentsline {paragraph}{Policy Gradient:}{35}{section*.39}\protected@file@percent }
\newlabel{eq:A}{{{A}}{35}{Policy Gradient:}{AMS.41}{}}
\newlabel{eq:A@cref}{{[equation][2147483647][]{A}}{[1][35][]35}}
\newlabel{eq:JG}{{14}{35}{Policy Gradient:}{equation.10.14}{}}
\newlabel{eq:JG@cref}{{[equation][14][]14}{[1][35][]35}}
\@writefile{toc}{\contentsline {paragraph}{From reward to policy update:}{37}{section*.42}\protected@file@percent }
\newlabel{eq:GHAT}{{16}{38}{From reward to policy update:}{equation.10.16}{}}
\newlabel{eq:GHAT@cref}{{[equation][16][]16}{[1][37][]38}}
\newlabel{eq:policy-grad}{{{Policy Gradient}}{38}{From reward to policy update:}{AMS.44}{}}
\newlabel{eq:policy-grad@cref}{{[equation][2147483647][]{Policy Gradient}}{[1][38][]38}}
\@writefile{toc}{\contentsline {paragraph}{Importance sampling based on old policy data.}{38}{section*.45}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Clipping.}{38}{section*.46}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Entropy bonus.}{39}{section*.47}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Full PPO loss.}{39}{section*.48}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Practical hint.}{39}{section*.49}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Regarding the meaning of concepts}{40}{section*.50}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.8}PPO-KL}{41}{subsection.10.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.9}DPO (Direct Preference Optimization)}{41}{subsection.10.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why do we assume $r$ follows the specific form?}{42}{section*.53}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.10}GPRO (Generalized Preference Optimization)}{43}{subsection.10.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The intuition:}{43}{section*.54}\protected@file@percent }
\newlabel{eq:change-measure}{{17}{43}{The intuition:}{equation.10.17}{}}
\newlabel{eq:change-measure@cref}{{[equation][17][]17}{[1][43][]43}}
\@writefile{toc}{\contentsline {paragraph}{The insight of GPRO:}{44}{section*.55}\protected@file@percent }
\newlabel{eq:org-1}{{18}{45}{The insight of GPRO:}{equation.10.18}{}}
\newlabel{eq:org-1@cref}{{[equation][18][]18}{[1][45][]45}}
\@writefile{toc}{\contentsline {paragraph}{Relation to DPO}{47}{section*.56}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.11}On Policy and Off Policy}{47}{subsection.10.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The idea of Q-learning:}{48}{section*.57}\protected@file@percent }
\newlabel{LastPage}{{}{48}{}{page.48}{}}
\xdef\lastpage@lastpage{48}
\xdef\lastpage@lastpageHy{48}
\gdef \@abspage@last{48}
